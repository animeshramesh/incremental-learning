{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo:\n",
    "- Experiment with normalization while creating triplets\n",
    "- Do you need dropout in the regressor network?\n",
    "- Simultaneous feature learning?\n",
    "- See when are results bad/equal/better? See which classes they correspond to.. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import cycle\n",
    "import random\n",
    "\n",
    "from utils import optimistic_restore, save\n",
    "import layers\n",
    "\n",
    "PWD = os.getcwd()\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(PWD, '..')))\n",
    "import pickle_utils\n",
    "import cifar_utils\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "HYPERPARAMS\n",
    "'''\n",
    "BATCH_SIZE = 10\n",
    "DATA_PATH = '/media/red/capstone/data/cifar-100/cifar-custom'\n",
    "LEARNING_RATE = 1e-4\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.99\n",
    "NUM_CLASSES = 40\n",
    "NUM_EPOCH = 100\n",
    "RANDOM_SEED = 1234\n",
    "SUMMARY_EVERY = 10\n",
    "VALIDATION_PERCENTAGE = 0.05\n",
    "SNAPSHOT_MAX = 10 # Keeps the last best 10 snapshots (best determined by validation accuracy)\n",
    "SNAPSHOT_DIR = '/media/red/capstone/snapshots/feature_extractor_vgg16'\n",
    "PRETRAINED_WEIGHT_FILE = '/media/red/capstone/pretrained_weights/vgg16_weights.npz'\n",
    "\n",
    "np.random.seed(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load custom CIFAR data. \n",
    "'''\n",
    "# cifar_raw = pickle_utils.load(DATA_PATH)\n",
    "custom_dataset = pickle_utils.load(DATA_PATH)\n",
    "\n",
    "data_x, data_y = [], []\n",
    "for label in custom_dataset['training'].keys():\n",
    "    for item in custom_dataset['training'][label]:\n",
    "        data_x.append(item) # 28 x 28 x 3\n",
    "        data_y.append(label) # 0-39\n",
    "data_x = np.stack(data_x).astype(np.float32)\n",
    "data_x = np.flip(data_x, axis=-1) # BGR\n",
    "data_y = np.stack(data_y).astype(np.int32)\n",
    "\n",
    "# Normalize x\n",
    "data_x = (data_x / 255.0) - 0.5\n",
    "\n",
    "def round_to(n, precision):\n",
    "    return int( n/precision+0.5 ) * precision\n",
    "\n",
    "n_total_data = data_x.shape[0]\n",
    "n_validation = round_to(VALIDATION_PERCENTAGE * n_total_data, BATCH_SIZE)\n",
    "batches_per_epoch = np.round((n_total_data - n_validation) / BATCH_SIZE)\n",
    "# Shuffle data\n",
    "random_indices = np.random.permutation(n_total_data)\n",
    "train_indices = cycle(random_indices[n_validation:])\n",
    "validation_indices = random_indices[:n_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Declare model\n",
    "'''\n",
    "class vgg16:\n",
    "    '''\n",
    "    VGG16 Model with ImageNet pretrained weight loader method\n",
    "    Weights can be downloaded from:\n",
    "    https://www.cs.toronto.edu/~frossard/vgg16/vgg16_weights.npz\n",
    "    '''\n",
    "\n",
    "    def __init__(self, x, y, phase):\n",
    "        '''\n",
    "        Sets up network enough to do a forward pass.\n",
    "        '''\n",
    "\n",
    "        \"\"\" init the model with hyper-parameters etc \"\"\"\n",
    "\n",
    "        # List used for loading weights from vgg16.npz (if necessary)\n",
    "        self.parameters = []\n",
    "        self.CONV_ACTIVATION = 'relu'\n",
    "        self.FC_ACTIVATION   = 'relu'\n",
    "\n",
    "        ########\n",
    "        # Misc #\n",
    "        ########\n",
    "        self.global_step = tf.get_variable('global_step', dtype=tf.int32, trainable=False,\n",
    "                        initializer=0)\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.IM_SHAPE = [224, 224, 3]\n",
    "\n",
    "        ####################\n",
    "        # I/O placeholders #\n",
    "        ####################\n",
    "        self.x = x\n",
    "        self.x.set_shape([None]+self.IM_SHAPE)\n",
    "        self.y = tf.to_int32(y)\n",
    "\n",
    "        ###############\n",
    "        # Main Layers #\n",
    "        ###############\n",
    "        with tf.variable_scope('conv_layers'):\n",
    "            self._convlayers()\n",
    "        with tf.variable_scope('fc_layers'):\n",
    "            self._fc_layers()\n",
    "\n",
    "        ######################\n",
    "        # Define Collections #\n",
    "        ######################\n",
    "        self.conv_trainable = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                \"conv_layers\")\n",
    "        self.fc_trainable = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                \"fc_layers\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        '''\n",
    "        Returns the count of correct classifications (Tensor).\n",
    "        '''\n",
    "        # Bool Tensor where 1 is correct and 0 is incorrect\n",
    "        correct = tf.nn.in_top_k(self.predictions, self.y, 1)\n",
    "        # Average them to get accuracy.  Must cast to a float32\n",
    "        self.accuracy = tf.reduce_mean(tf.to_float(correct))\n",
    "        return self.accuracy\n",
    "\n",
    "    #####################\n",
    "    # Private Functions #\n",
    "    #####################\n",
    "    def _convlayers(self):\n",
    "        '''\n",
    "        All conv and pooling layers of VGG16\n",
    "        '''\n",
    "        # zero-mean input; resizing has to be done beforehand for uniform tensor shape\n",
    "        with tf.variable_scope('preprocess'):\n",
    "            mean = tf.constant([123.68, 116.779, 103.939],\n",
    "                    dtype=tf.float32,\n",
    "                    shape=[1, 1, 1, 3],\n",
    "                    name='img_mean')\n",
    "            self.images = self.x*255.0 - mean\n",
    "\n",
    "        # conv1_1\n",
    "        self.conv1_1, weights, biases = layers.conv2d(name='conv1_1',\n",
    "                input=self.images,\n",
    "                shape=(3,3,3,64),\n",
    "                padding='SAME',\n",
    "                strides = [1,1,1,1],\n",
    "                activation=self.CONV_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # conv1_2\n",
    "        self.conv1_2, weights, biases = layers.conv2d(name='conv1_2',\n",
    "                input=self.conv1_1,\n",
    "                shape=(3,3,64,64),\n",
    "                padding='SAME',\n",
    "                strides = [1,1,1,1],\n",
    "                activation=self.CONV_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # pool1\n",
    "        self.pool1 = tf.nn.max_pool(self.conv1_2,\n",
    "                ksize=[1, 2, 2, 1],\n",
    "                strides=[1, 2, 2, 1],\n",
    "                padding='SAME',\n",
    "                name='pool1')\n",
    "\n",
    "        # conv2_1\n",
    "        self.conv2_1, weights, biases = layers.conv2d(name='conv2_1',\n",
    "                input=self.pool1,\n",
    "                shape=(3,3,64,128),\n",
    "                padding='SAME',\n",
    "                strides = [1,1,1,1],\n",
    "                activation=self.CONV_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # conv2_2\n",
    "        self.conv2_2, weights, biases = layers.conv2d(name='conv2_2',\n",
    "                input=self.conv2_1,\n",
    "                shape=(3,3,128,128),\n",
    "                padding='SAME',\n",
    "                strides = [1,1,1,1],\n",
    "                activation=self.CONV_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # pool2\n",
    "        self.pool2 = tf.nn.max_pool(self.conv2_2,\n",
    "                ksize=[1, 2, 2, 1],\n",
    "                strides=[1, 2, 2, 1],\n",
    "                padding='SAME',\n",
    "                name='pool2')\n",
    "\n",
    "        # conv3_1\n",
    "        self.conv3_1, weights, biases = layers.conv2d(name='conv3_1',\n",
    "                input=self.pool2,\n",
    "                shape=(3,3,128,256),\n",
    "                padding='SAME',\n",
    "                strides = [1,1,1,1],\n",
    "                activation=self.CONV_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # conv3_2\n",
    "        self.conv3_2, weights, biases = layers.conv2d(name='conv3_2',\n",
    "                input=self.conv3_1,\n",
    "                shape=(3,3,256,256),\n",
    "                padding='SAME',\n",
    "                strides = [1,1,1,1],\n",
    "                activation=self.CONV_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # conv3_3\n",
    "        self.conv3_3, weights, biases = layers.conv2d(name='conv3_3',\n",
    "                input=self.conv3_2,\n",
    "                shape=(3,3,256,256),\n",
    "                padding='SAME',\n",
    "                strides = [1,1,1,1],\n",
    "                activation=self.CONV_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # pool3\n",
    "        self.pool3 = tf.nn.max_pool(self.conv3_3,\n",
    "                ksize=[1, 2, 2, 1],\n",
    "                strides=[1, 2, 2, 1],\n",
    "                padding='SAME',\n",
    "                name='pool3')\n",
    "\n",
    "        # conv4_1\n",
    "        self.conv4_1, weights, biases = layers.conv2d(name='conv4_1',\n",
    "                input=self.pool3,\n",
    "                shape=(3,3,256,512),\n",
    "                padding='SAME',\n",
    "                strides = [1,1,1,1],\n",
    "                activation=self.CONV_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # conv4_2\n",
    "        self.conv4_2, weights, biases = layers.conv2d(name='conv4_2',\n",
    "                input=self.conv4_1,\n",
    "                shape=(3,3,512,512),\n",
    "                padding='SAME',\n",
    "                strides = [1,1,1,1],\n",
    "                activation=self.CONV_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # conv4_3\n",
    "        self.conv4_3, weights, biases = layers.conv2d(name='conv4_3',\n",
    "                input=self.conv4_2,\n",
    "                shape=(3,3,512,512),\n",
    "                padding='SAME',\n",
    "                strides = [1,1,1,1],\n",
    "                activation=self.CONV_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # pool4\n",
    "        self.pool4 = tf.nn.max_pool(self.conv4_3,\n",
    "                ksize=[1, 2, 2, 1],\n",
    "                strides=[1, 2, 2, 1],\n",
    "                padding='SAME',\n",
    "                name='pool4')\n",
    "\n",
    "        # conv5_1\n",
    "        self.conv5_1, weights, biases = layers.conv2d(name='conv5_1',\n",
    "                input=self.pool4,\n",
    "                shape=(3,3,512,512),\n",
    "                padding='SAME',\n",
    "                strides = [1,1,1,1],\n",
    "                activation=self.CONV_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # conv5_2\n",
    "        self.conv5_2, weights, biases = layers.conv2d(name='conv5_2',\n",
    "                input=self.conv5_1,\n",
    "                shape=(3,3,512,512),\n",
    "                padding='SAME',\n",
    "                strides = [1,1,1,1],\n",
    "                activation=self.CONV_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # conv5_3\n",
    "        self.conv5_3, weights, biases = layers.conv2d(name='conv5_3',\n",
    "                input=self.conv5_2,\n",
    "                shape=(3,3,512,512),\n",
    "                padding='SAME',\n",
    "                strides = [1,1,1,1],\n",
    "                activation=self.CONV_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # pool5\n",
    "        self.pool5 = tf.nn.max_pool(self.conv5_3,\n",
    "                ksize=[1, 2, 2, 1],\n",
    "                strides=[1, 2, 2, 1],\n",
    "                padding='SAME',\n",
    "                name='pool5')\n",
    "\n",
    "    def _fc_layers(self):\n",
    "        '''\n",
    "        All FC layers of VGG16 (+custom layers)\n",
    "        '''\n",
    "        # fc1\n",
    "        self.fc1, weights, biases = layers.fc(name='fc1',\n",
    "                input=tf.contrib.layers.flatten(self.pool5),\n",
    "                units=4096,\n",
    "                activation=self.FC_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # fc2\n",
    "        self.fc2, weights, biases = layers.fc(name='fc2',\n",
    "                input=self.fc1,\n",
    "                units=4096,\n",
    "                activation=self.FC_ACTIVATION)\n",
    "        self.parameters += [weights, biases]\n",
    "\n",
    "        # fc3\n",
    "        self.fc3, weights, biases = layers.fc(name='fc3',\n",
    "                input=self.fc2,\n",
    "                units=NUM_CLASSES,\n",
    "                activation='linear')\n",
    "\n",
    "    def load_pretrained_weights(self, sess):\n",
    "        '''\n",
    "        Load Pretrained VGG16 weights from .npz file\n",
    "        (weights converted from Caffe)\n",
    "        To only be used when no TensorFlow Snapshot is avaialable.\n",
    "        Assumes layers are properly added to self.parameters.\n",
    "        '''\n",
    "        print(\"Loading Imagenet Weights.\")\n",
    "\n",
    "        weights = np.load(PRETRAINED_WEIGHT_FILE)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            print(i, k, np.shape(weights[k]))\n",
    "            try:\n",
    "                sess.run(self.parameters[i].assign(weights[k]))\n",
    "            except:\n",
    "                print(\"%s layer not found.\" % k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Imagenet Weights.\n",
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n",
      "9 conv3_1_b (256,)\n",
      "10 conv3_2_W (3, 3, 256, 256)\n",
      "11 conv3_2_b (256,)\n",
      "12 conv3_3_W (3, 3, 256, 256)\n",
      "13 conv3_3_b (256,)\n",
      "14 conv4_1_W (3, 3, 256, 512)\n",
      "15 conv4_1_b (512,)\n",
      "16 conv4_2_W (3, 3, 512, 512)\n",
      "17 conv4_2_b (512,)\n",
      "18 conv4_3_W (3, 3, 512, 512)\n",
      "19 conv4_3_b (512,)\n",
      "20 conv5_1_W (3, 3, 512, 512)\n",
      "21 conv5_1_b (512,)\n",
      "22 conv5_2_W (3, 3, 512, 512)\n",
      "23 conv5_2_b (512,)\n",
      "24 conv5_3_W (3, 3, 512, 512)\n",
      "25 conv5_3_b (512,)\n",
      "26 fc6_W (25088, 4096)\n",
      "27 fc6_b (4096,)\n",
      "28 fc7_W (4096, 4096)\n",
      "29 fc7_b (4096,)\n",
      "30 fc8_W (4096, 1000)\n",
      "fc8_W layer not found.\n",
      "31 fc8_b (1000,)\n",
      "fc8_b layer not found.\n",
      "step 10 \t loss = 4.016, train_acc = 0.100 (3.284 sec/step)\n",
      "step 20 \t loss = 3.802, train_acc = 0.000 (3.264 sec/step)\n",
      "step 30 \t loss = 3.768, train_acc = 0.100 (3.209 sec/step)\n",
      "step 40 \t loss = 3.426, train_acc = 0.200 (3.224 sec/step)\n",
      "step 50 \t loss = 4.138, train_acc = 0.000 (3.166 sec/step)\n",
      "step 60 \t loss = 3.674, train_acc = 0.000 (3.184 sec/step)\n",
      "step 70 \t loss = 3.752, train_acc = 0.000 (3.290 sec/step)\n",
      "step 80 \t loss = 3.775, train_acc = 0.000 (3.189 sec/step)\n",
      "step 90 \t loss = 3.769, train_acc = 0.000 (3.203 sec/step)\n",
      "step 100 \t loss = 3.721, train_acc = 0.000 (3.228 sec/step)\n",
      "step 110 \t loss = 3.656, train_acc = 0.100 (3.223 sec/step)\n",
      "step 120 \t loss = 3.578, train_acc = 0.100 (3.219 sec/step)\n",
      "step 130 \t loss = 3.286, train_acc = 0.200 (3.207 sec/step)\n",
      "step 140 \t loss = 3.610, train_acc = 0.000 (3.215 sec/step)\n",
      "step 150 \t loss = 3.933, train_acc = 0.000 (3.190 sec/step)\n",
      "step 160 \t loss = 3.712, train_acc = 0.000 (3.251 sec/step)\n",
      "step 170 \t loss = 3.366, train_acc = 0.100 (3.182 sec/step)\n",
      "step 180 \t loss = 3.710, train_acc = 0.000 (3.188 sec/step)\n",
      "step 190 \t loss = 3.553, train_acc = 0.000 (3.170 sec/step)\n",
      "step 200 \t loss = 3.552, train_acc = 0.100 (3.180 sec/step)\n",
      "step 210 \t loss = 4.046, train_acc = 0.000 (3.208 sec/step)\n",
      "step 220 \t loss = 3.622, train_acc = 0.100 (3.216 sec/step)\n",
      "step 230 \t loss = 3.503, train_acc = 0.000 (3.230 sec/step)\n",
      "step 240 \t loss = 3.455, train_acc = 0.100 (3.156 sec/step)\n",
      "step 250 \t loss = 3.724, train_acc = 0.100 (3.220 sec/step)\n",
      "step 260 \t loss = 3.593, train_acc = 0.000 (3.177 sec/step)\n",
      "step 270 \t loss = 3.433, train_acc = 0.000 (3.152 sec/step)\n",
      "step 280 \t loss = 3.947, train_acc = 0.000 (3.156 sec/step)\n",
      "step 290 \t loss = 3.426, train_acc = 0.100 (3.144 sec/step)\n",
      "step 300 \t loss = 3.620, train_acc = 0.100 (3.182 sec/step)\n",
      "step 310 \t loss = 3.490, train_acc = 0.100 (3.255 sec/step)\n",
      "step 320 \t loss = 3.755, train_acc = 0.100 (3.203 sec/step)\n",
      "step 330 \t loss = 3.517, train_acc = 0.000 (3.319 sec/step)\n",
      "step 340 \t loss = 3.490, train_acc = 0.000 (3.245 sec/step)\n",
      "step 350 \t loss = 3.225, train_acc = 0.200 (3.224 sec/step)\n",
      "step 360 \t loss = 3.388, train_acc = 0.100 (3.191 sec/step)\n",
      "step 370 \t loss = 3.433, train_acc = 0.100 (3.255 sec/step)\n",
      "step 380 \t loss = 3.195, train_acc = 0.100 (3.181 sec/step)\n",
      "step 390 \t loss = 3.656, train_acc = 0.000 (3.209 sec/step)\n",
      "step 400 \t loss = 3.532, train_acc = 0.200 (3.214 sec/step)\n",
      "step 410 \t loss = 3.477, train_acc = 0.200 (3.154 sec/step)\n",
      "step 420 \t loss = 3.132, train_acc = 0.300 (3.159 sec/step)\n",
      "step 430 \t loss = 3.212, train_acc = 0.100 (3.225 sec/step)\n",
      "step 440 \t loss = 3.532, train_acc = 0.000 (3.313 sec/step)\n",
      "step 450 \t loss = 3.489, train_acc = 0.100 (3.160 sec/step)\n",
      "step 460 \t loss = 3.127, train_acc = 0.300 (3.205 sec/step)\n",
      "step 470 \t loss = 2.845, train_acc = 0.100 (3.347 sec/step)\n",
      "step 480 \t loss = 3.077, train_acc = 0.200 (3.174 sec/step)\n",
      "step 490 \t loss = 3.702, train_acc = 0.000 (3.218 sec/step)\n",
      "step 500 \t loss = 3.563, train_acc = 0.100 (3.164 sec/step)\n",
      "step 510 \t loss = 3.040, train_acc = 0.200 (3.171 sec/step)\n",
      "step 520 \t loss = 3.210, train_acc = 0.100 (3.217 sec/step)\n",
      "step 530 \t loss = 2.704, train_acc = 0.300 (3.210 sec/step)\n",
      "step 540 \t loss = 3.087, train_acc = 0.200 (3.171 sec/step)\n",
      "step 550 \t loss = 3.319, train_acc = 0.100 (3.194 sec/step)\n",
      "step 560 \t loss = 3.750, train_acc = 0.000 (3.174 sec/step)\n",
      "step 570 \t loss = 2.802, train_acc = 0.200 (3.162 sec/step)\n",
      "step 580 \t loss = 3.850, train_acc = 0.000 (3.173 sec/step)\n",
      "step 590 \t loss = 3.427, train_acc = 0.000 (3.226 sec/step)\n",
      "step 600 \t loss = 3.588, train_acc = 0.200 (3.200 sec/step)\n",
      "step 610 \t loss = 3.514, train_acc = 0.100 (3.188 sec/step)\n",
      "step 620 \t loss = 3.360, train_acc = 0.100 (3.212 sec/step)\n",
      "step 630 \t loss = 3.234, train_acc = 0.100 (3.193 sec/step)\n",
      "step 640 \t loss = 2.916, train_acc = 0.300 (3.256 sec/step)\n",
      "step 650 \t loss = 3.536, train_acc = 0.000 (3.141 sec/step)\n",
      "step 660 \t loss = 3.430, train_acc = 0.100 (3.249 sec/step)\n",
      "step 670 \t loss = 3.839, train_acc = 0.000 (3.186 sec/step)\n",
      "step 680 \t loss = 3.264, train_acc = 0.000 (3.204 sec/step)\n",
      "step 690 \t loss = 3.654, train_acc = 0.000 (3.242 sec/step)\n",
      "step 700 \t loss = 3.720, train_acc = 0.000 (3.207 sec/step)\n",
      "step 710 \t loss = 3.079, train_acc = 0.100 (3.200 sec/step)\n",
      "step 720 \t loss = 2.818, train_acc = 0.300 (3.211 sec/step)\n",
      "step 730 \t loss = 2.916, train_acc = 0.200 (3.198 sec/step)\n",
      "step 740 \t loss = 2.922, train_acc = 0.100 (3.223 sec/step)\n",
      "step 750 \t loss = 3.265, train_acc = 0.000 (3.206 sec/step)\n",
      "step 760 \t loss = 3.681, train_acc = 0.000 (3.170 sec/step)\n",
      "step 770 \t loss = 3.102, train_acc = 0.100 (3.218 sec/step)\n",
      "step 780 \t loss = 2.964, train_acc = 0.200 (3.238 sec/step)\n",
      "step 790 \t loss = 2.636, train_acc = 0.200 (3.207 sec/step)\n",
      "step 800 \t loss = 3.001, train_acc = 0.300 (3.300 sec/step)\n",
      "step 810 \t loss = 3.001, train_acc = 0.100 (3.165 sec/step)\n",
      "step 820 \t loss = 2.690, train_acc = 0.300 (3.348 sec/step)\n",
      "step 830 \t loss = 3.011, train_acc = 0.200 (3.240 sec/step)\n",
      "step 840 \t loss = 3.133, train_acc = 0.100 (3.263 sec/step)\n",
      "step 850 \t loss = 3.634, train_acc = 0.100 (3.159 sec/step)\n",
      "step 860 \t loss = 2.048, train_acc = 0.400 (3.184 sec/step)\n",
      "step 870 \t loss = 3.360, train_acc = 0.100 (3.275 sec/step)\n",
      "step 880 \t loss = 3.805, train_acc = 0.100 (3.187 sec/step)\n",
      "step 890 \t loss = 3.300, train_acc = 0.100 (3.217 sec/step)\n",
      "step 900 \t loss = 3.355, train_acc = 0.000 (3.216 sec/step)\n",
      "step 910 \t loss = 3.394, train_acc = 0.000 (3.231 sec/step)\n",
      "step 920 \t loss = 2.625, train_acc = 0.500 (3.168 sec/step)\n",
      "step 930 \t loss = 3.215, train_acc = 0.100 (3.206 sec/step)\n",
      "step 940 \t loss = 3.373, train_acc = 0.200 (3.197 sec/step)\n",
      "step 950 \t loss = 2.538, train_acc = 0.300 (3.195 sec/step)\n",
      "step 960 \t loss = 2.825, train_acc = 0.200 (3.185 sec/step)\n",
      "step 970 \t loss = 3.232, train_acc = 0.000 (3.180 sec/step)\n",
      "step 980 \t loss = 2.790, train_acc = 0.200 (3.199 sec/step)\n",
      "step 990 \t loss = 2.992, train_acc = 0.100 (3.212 sec/step)\n",
      "step 1000 \t loss = 2.318, train_acc = 0.500 (3.170 sec/step)\n",
      "step 1010 \t loss = 3.109, train_acc = 0.200 (3.252 sec/step)\n",
      "step 1020 \t loss = 2.946, train_acc = 0.100 (3.220 sec/step)\n",
      "step 1030 \t loss = 3.068, train_acc = 0.100 (3.191 sec/step)\n",
      "step 1040 \t loss = 3.970, train_acc = 0.100 (3.206 sec/step)\n",
      "step 1050 \t loss = 3.166, train_acc = 0.300 (3.188 sec/step)\n",
      "step 1060 \t loss = 3.055, train_acc = 0.000 (3.206 sec/step)\n",
      "step 1070 \t loss = 3.163, train_acc = 0.200 (3.235 sec/step)\n",
      "step 1080 \t loss = 3.314, train_acc = 0.000 (3.247 sec/step)\n",
      "step 1090 \t loss = 3.187, train_acc = 0.000 (3.179 sec/step)\n",
      "step 1100 \t loss = 3.028, train_acc = 0.200 (3.214 sec/step)\n",
      "step 1110 \t loss = 2.852, train_acc = 0.200 (3.208 sec/step)\n",
      "step 1120 \t loss = 2.615, train_acc = 0.300 (3.223 sec/step)\n",
      "step 1130 \t loss = 2.726, train_acc = 0.300 (3.235 sec/step)\n",
      "step 1140 \t loss = 2.974, train_acc = 0.200 (3.185 sec/step)\n",
      "step 1150 \t loss = 2.674, train_acc = 0.300 (3.200 sec/step)\n",
      "step 1160 \t loss = 2.860, train_acc = 0.200 (3.208 sec/step)\n",
      "step 1170 \t loss = 3.027, train_acc = 0.000 (3.157 sec/step)\n",
      "step 1180 \t loss = 2.855, train_acc = 0.500 (3.214 sec/step)\n",
      "step 1190 \t loss = 2.548, train_acc = 0.100 (3.245 sec/step)\n",
      "step 1200 \t loss = 2.321, train_acc = 0.300 (3.156 sec/step)\n",
      "step 1210 \t loss = 3.606, train_acc = 0.100 (3.222 sec/step)\n",
      "step 1220 \t loss = 2.553, train_acc = 0.500 (3.243 sec/step)\n",
      "step 1230 \t loss = 2.841, train_acc = 0.000 (3.244 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1240 \t loss = 2.155, train_acc = 0.400 (3.167 sec/step)\n",
      "step 1250 \t loss = 2.729, train_acc = 0.200 (3.202 sec/step)\n",
      "step 1260 \t loss = 2.669, train_acc = 0.400 (3.197 sec/step)\n",
      "step 1270 \t loss = 3.279, train_acc = 0.200 (3.172 sec/step)\n",
      "step 1280 \t loss = 2.550, train_acc = 0.200 (3.194 sec/step)\n",
      "step 1290 \t loss = 3.035, train_acc = 0.200 (3.181 sec/step)\n",
      "step 1300 \t loss = 2.790, train_acc = 0.100 (3.197 sec/step)\n",
      "step 1310 \t loss = 3.025, train_acc = 0.100 (3.171 sec/step)\n",
      "step 1320 \t loss = 2.360, train_acc = 0.200 (3.176 sec/step)\n",
      "step 1330 \t loss = 2.270, train_acc = 0.400 (3.234 sec/step)\n",
      "step 1340 \t loss = 2.807, train_acc = 0.100 (3.195 sec/step)\n",
      "step 1350 \t loss = 2.853, train_acc = 0.200 (3.172 sec/step)\n",
      "step 1360 \t loss = 3.099, train_acc = 0.200 (3.178 sec/step)\n",
      "step 1370 \t loss = 2.313, train_acc = 0.200 (3.228 sec/step)\n",
      "step 1380 \t loss = 2.563, train_acc = 0.200 (3.187 sec/step)\n",
      "step 1390 \t loss = 3.341, train_acc = 0.100 (3.209 sec/step)\n",
      "step 1400 \t loss = 2.515, train_acc = 0.300 (3.189 sec/step)\n",
      "step 1410 \t loss = 2.953, train_acc = 0.200 (3.240 sec/step)\n",
      "step 1420 \t loss = 2.078, train_acc = 0.500 (3.150 sec/step)\n",
      "step 1430 \t loss = 2.927, train_acc = 0.100 (3.222 sec/step)\n",
      "step 1440 \t loss = 2.493, train_acc = 0.400 (3.218 sec/step)\n",
      "step 1450 \t loss = 3.245, train_acc = 0.100 (3.206 sec/step)\n",
      "step 1460 \t loss = 1.968, train_acc = 0.700 (3.209 sec/step)\n",
      "step 1470 \t loss = 2.869, train_acc = 0.500 (3.243 sec/step)\n",
      "step 1480 \t loss = 3.286, train_acc = 0.000 (3.152 sec/step)\n",
      "step 1490 \t loss = 3.555, train_acc = 0.000 (3.353 sec/step)\n",
      "step 1500 \t loss = 3.006, train_acc = 0.200 (3.190 sec/step)\n",
      "step 1510 \t loss = 2.795, train_acc = 0.100 (3.200 sec/step)\n",
      "step 1520 \t loss = 2.768, train_acc = 0.200 (3.232 sec/step)\n",
      "step 1530 \t loss = 2.351, train_acc = 0.300 (3.152 sec/step)\n",
      "step 1540 \t loss = 2.531, train_acc = 0.200 (3.194 sec/step)\n",
      "step 1550 \t loss = 2.397, train_acc = 0.400 (3.223 sec/step)\n",
      "step 1560 \t loss = 2.498, train_acc = 0.300 (3.224 sec/step)\n",
      "step 1570 \t loss = 3.230, train_acc = 0.000 (3.226 sec/step)\n",
      "step 1580 \t loss = 2.705, train_acc = 0.100 (3.279 sec/step)\n",
      "step 1590 \t loss = 2.631, train_acc = 0.200 (3.189 sec/step)\n",
      "step 1600 \t loss = 3.098, train_acc = 0.200 (3.193 sec/step)\n",
      "step 1610 \t loss = 2.744, train_acc = 0.300 (3.246 sec/step)\n",
      "step 1620 \t loss = 2.029, train_acc = 0.400 (3.227 sec/step)\n",
      "step 1630 \t loss = 3.586, train_acc = 0.100 (3.206 sec/step)\n",
      "step 1640 \t loss = 2.784, train_acc = 0.200 (3.224 sec/step)\n",
      "step 1650 \t loss = 2.958, train_acc = 0.200 (3.173 sec/step)\n",
      "step 1660 \t loss = 2.740, train_acc = 0.200 (3.211 sec/step)\n",
      "step 1670 \t loss = 2.262, train_acc = 0.500 (3.221 sec/step)\n",
      "step 1680 \t loss = 2.287, train_acc = 0.300 (3.285 sec/step)\n",
      "step 1690 \t loss = 3.078, train_acc = 0.300 (3.205 sec/step)\n",
      "step 1700 \t loss = 1.848, train_acc = 0.500 (3.181 sec/step)\n",
      "step 1710 \t loss = 2.920, train_acc = 0.200 (3.256 sec/step)\n",
      "step 1720 \t loss = 1.947, train_acc = 0.400 (3.240 sec/step)\n",
      "step 1730 \t loss = 3.077, train_acc = 0.400 (3.228 sec/step)\n",
      "step 1740 \t loss = 2.631, train_acc = 0.400 (3.232 sec/step)\n",
      "step 1750 \t loss = 2.554, train_acc = 0.300 (3.230 sec/step)\n",
      "step 1760 \t loss = 2.358, train_acc = 0.500 (3.331 sec/step)\n",
      "step 1770 \t loss = 1.574, train_acc = 0.600 (3.189 sec/step)\n",
      "step 1780 \t loss = 2.250, train_acc = 0.300 (3.224 sec/step)\n",
      "step 1790 \t loss = 2.314, train_acc = 0.400 (3.180 sec/step)\n",
      "step 1800 \t loss = 3.102, train_acc = 0.100 (3.197 sec/step)\n",
      "step 1810 \t loss = 2.620, train_acc = 0.300 (3.193 sec/step)\n",
      "step 1820 \t loss = 2.680, train_acc = 0.200 (3.180 sec/step)\n",
      "step 1830 \t loss = 2.953, train_acc = 0.300 (3.204 sec/step)\n",
      "step 1840 \t loss = 3.217, train_acc = 0.200 (3.210 sec/step)\n",
      "step 1850 \t loss = 2.224, train_acc = 0.500 (3.215 sec/step)\n",
      "step 1860 \t loss = 2.762, train_acc = 0.200 (3.191 sec/step)\n",
      "step 1870 \t loss = 2.089, train_acc = 0.500 (3.205 sec/step)\n",
      "step 1880 \t loss = 2.195, train_acc = 0.300 (3.195 sec/step)\n",
      "step 1890 \t loss = 3.014, train_acc = 0.300 (3.212 sec/step)\n",
      "VALIDATION \t acc = 0.273 (3.651 sec)\n",
      "New Best Accuracy 0.273 > Old Best 0.000.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 1900 \t loss = 3.152, train_acc = 0.200 (3.215 sec/step)\n",
      "step 1910 \t loss = 3.421, train_acc = 0.100 (3.197 sec/step)\n",
      "step 1920 \t loss = 2.026, train_acc = 0.500 (3.186 sec/step)\n",
      "step 1930 \t loss = 2.937, train_acc = 0.100 (3.226 sec/step)\n",
      "step 1940 \t loss = 2.043, train_acc = 0.400 (3.185 sec/step)\n",
      "step 1950 \t loss = 3.006, train_acc = 0.100 (3.242 sec/step)\n",
      "step 1960 \t loss = 1.940, train_acc = 0.400 (3.218 sec/step)\n",
      "step 1970 \t loss = 2.030, train_acc = 0.500 (3.227 sec/step)\n",
      "step 1980 \t loss = 3.180, train_acc = 0.100 (3.205 sec/step)\n",
      "step 1990 \t loss = 2.659, train_acc = 0.300 (3.184 sec/step)\n",
      "step 2000 \t loss = 2.071, train_acc = 0.500 (3.238 sec/step)\n",
      "step 2010 \t loss = 2.068, train_acc = 0.400 (3.202 sec/step)\n",
      "step 2020 \t loss = 2.556, train_acc = 0.300 (3.205 sec/step)\n",
      "step 2030 \t loss = 2.160, train_acc = 0.200 (3.193 sec/step)\n",
      "step 2040 \t loss = 2.912, train_acc = 0.100 (3.216 sec/step)\n",
      "step 2050 \t loss = 2.773, train_acc = 0.100 (3.169 sec/step)\n",
      "step 2060 \t loss = 2.354, train_acc = 0.300 (3.226 sec/step)\n",
      "step 2070 \t loss = 2.281, train_acc = 0.500 (3.185 sec/step)\n",
      "step 2080 \t loss = 2.520, train_acc = 0.100 (3.251 sec/step)\n",
      "step 2090 \t loss = 2.112, train_acc = 0.300 (3.191 sec/step)\n",
      "step 2100 \t loss = 2.600, train_acc = 0.200 (3.175 sec/step)\n",
      "step 2110 \t loss = 2.616, train_acc = 0.300 (3.191 sec/step)\n",
      "step 2120 \t loss = 2.661, train_acc = 0.200 (3.199 sec/step)\n",
      "step 2130 \t loss = 1.727, train_acc = 0.600 (3.196 sec/step)\n",
      "step 2140 \t loss = 2.098, train_acc = 0.400 (3.181 sec/step)\n",
      "step 2150 \t loss = 3.006, train_acc = 0.200 (3.220 sec/step)\n",
      "step 2160 \t loss = 2.458, train_acc = 0.200 (3.252 sec/step)\n",
      "step 2170 \t loss = 2.696, train_acc = 0.400 (3.206 sec/step)\n",
      "step 2180 \t loss = 2.523, train_acc = 0.200 (3.182 sec/step)\n",
      "step 2190 \t loss = 1.894, train_acc = 0.400 (3.195 sec/step)\n",
      "step 2200 \t loss = 2.873, train_acc = 0.400 (3.146 sec/step)\n",
      "step 2210 \t loss = 2.514, train_acc = 0.000 (3.247 sec/step)\n",
      "step 2220 \t loss = 2.848, train_acc = 0.200 (3.242 sec/step)\n",
      "step 2230 \t loss = 2.159, train_acc = 0.400 (3.221 sec/step)\n",
      "step 2240 \t loss = 2.047, train_acc = 0.500 (3.188 sec/step)\n",
      "step 2250 \t loss = 1.657, train_acc = 0.500 (3.154 sec/step)\n",
      "step 2260 \t loss = 2.464, train_acc = 0.200 (3.247 sec/step)\n",
      "step 2270 \t loss = 1.516, train_acc = 0.500 (3.189 sec/step)\n",
      "step 2280 \t loss = 2.279, train_acc = 0.400 (3.226 sec/step)\n",
      "step 2290 \t loss = 1.505, train_acc = 0.500 (3.171 sec/step)\n",
      "step 2300 \t loss = 3.246, train_acc = 0.300 (3.176 sec/step)\n",
      "step 2310 \t loss = 2.740, train_acc = 0.300 (3.232 sec/step)\n",
      "step 2320 \t loss = 1.847, train_acc = 0.500 (3.153 sec/step)\n",
      "step 2330 \t loss = 2.013, train_acc = 0.700 (3.243 sec/step)\n",
      "step 2340 \t loss = 2.089, train_acc = 0.500 (3.198 sec/step)\n",
      "step 2350 \t loss = 2.228, train_acc = 0.300 (3.216 sec/step)\n",
      "step 2360 \t loss = 2.311, train_acc = 0.200 (3.174 sec/step)\n",
      "step 2370 \t loss = 1.985, train_acc = 0.300 (3.191 sec/step)\n",
      "step 2380 \t loss = 1.929, train_acc = 0.400 (3.227 sec/step)\n",
      "step 2390 \t loss = 3.008, train_acc = 0.200 (3.177 sec/step)\n",
      "step 2400 \t loss = 2.326, train_acc = 0.400 (3.194 sec/step)\n",
      "step 2410 \t loss = 1.938, train_acc = 0.400 (3.213 sec/step)\n",
      "step 2420 \t loss = 1.980, train_acc = 0.400 (3.250 sec/step)\n",
      "step 2430 \t loss = 2.520, train_acc = 0.100 (3.204 sec/step)\n",
      "step 2440 \t loss = 1.911, train_acc = 0.400 (3.189 sec/step)\n",
      "step 2450 \t loss = 2.433, train_acc = 0.400 (3.235 sec/step)\n",
      "step 2460 \t loss = 2.551, train_acc = 0.400 (3.226 sec/step)\n",
      "step 2470 \t loss = 1.775, train_acc = 0.500 (3.239 sec/step)\n",
      "step 2480 \t loss = 3.057, train_acc = 0.300 (3.234 sec/step)\n",
      "step 2490 \t loss = 2.502, train_acc = 0.300 (3.250 sec/step)\n",
      "step 2500 \t loss = 2.681, train_acc = 0.300 (3.250 sec/step)\n",
      "step 2510 \t loss = 1.648, train_acc = 0.600 (3.248 sec/step)\n",
      "step 2520 \t loss = 1.725, train_acc = 0.400 (3.216 sec/step)\n",
      "step 2530 \t loss = 3.144, train_acc = 0.100 (3.195 sec/step)\n",
      "step 2540 \t loss = 1.538, train_acc = 0.400 (3.203 sec/step)\n",
      "step 2550 \t loss = 2.843, train_acc = 0.300 (3.197 sec/step)\n",
      "step 2560 \t loss = 1.772, train_acc = 0.400 (3.221 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2570 \t loss = 3.213, train_acc = 0.000 (3.158 sec/step)\n",
      "step 2580 \t loss = 1.551, train_acc = 0.400 (3.254 sec/step)\n",
      "step 2590 \t loss = 2.392, train_acc = 0.200 (3.215 sec/step)\n",
      "step 2600 \t loss = 3.312, train_acc = 0.100 (3.195 sec/step)\n",
      "step 2610 \t loss = 1.465, train_acc = 0.600 (3.203 sec/step)\n",
      "step 2620 \t loss = 1.292, train_acc = 0.500 (3.197 sec/step)\n",
      "step 2630 \t loss = 2.011, train_acc = 0.500 (3.230 sec/step)\n",
      "step 2640 \t loss = 1.273, train_acc = 0.600 (3.174 sec/step)\n",
      "step 2650 \t loss = 2.261, train_acc = 0.300 (3.204 sec/step)\n",
      "step 2660 \t loss = 2.621, train_acc = 0.100 (3.203 sec/step)\n",
      "step 2670 \t loss = 1.360, train_acc = 0.700 (3.283 sec/step)\n",
      "step 2680 \t loss = 2.424, train_acc = 0.300 (3.209 sec/step)\n",
      "step 2690 \t loss = 1.961, train_acc = 0.400 (3.307 sec/step)\n",
      "step 2700 \t loss = 2.408, train_acc = 0.300 (3.200 sec/step)\n",
      "step 2710 \t loss = 2.306, train_acc = 0.200 (3.248 sec/step)\n",
      "step 2720 \t loss = 1.642, train_acc = 0.600 (3.231 sec/step)\n",
      "step 2730 \t loss = 1.951, train_acc = 0.500 (3.240 sec/step)\n",
      "step 2740 \t loss = 2.783, train_acc = 0.300 (3.236 sec/step)\n",
      "step 2750 \t loss = 2.259, train_acc = 0.400 (3.200 sec/step)\n",
      "step 2760 \t loss = 1.215, train_acc = 0.600 (3.213 sec/step)\n",
      "step 2770 \t loss = 2.373, train_acc = 0.200 (3.208 sec/step)\n",
      "step 2780 \t loss = 2.740, train_acc = 0.400 (3.241 sec/step)\n",
      "step 2790 \t loss = 2.114, train_acc = 0.300 (3.327 sec/step)\n",
      "step 2800 \t loss = 2.909, train_acc = 0.000 (3.167 sec/step)\n",
      "step 2810 \t loss = 2.828, train_acc = 0.200 (3.263 sec/step)\n",
      "step 2820 \t loss = 1.774, train_acc = 0.400 (3.190 sec/step)\n",
      "step 2830 \t loss = 2.093, train_acc = 0.300 (3.336 sec/step)\n",
      "step 2840 \t loss = 2.376, train_acc = 0.300 (3.228 sec/step)\n",
      "step 2850 \t loss = 1.642, train_acc = 0.600 (3.251 sec/step)\n",
      "step 2860 \t loss = 1.533, train_acc = 0.500 (3.224 sec/step)\n",
      "step 2870 \t loss = 1.664, train_acc = 0.400 (3.238 sec/step)\n",
      "step 2880 \t loss = 1.437, train_acc = 0.700 (3.166 sec/step)\n",
      "step 2890 \t loss = 2.497, train_acc = 0.200 (3.187 sec/step)\n",
      "step 2900 \t loss = 1.609, train_acc = 0.600 (3.217 sec/step)\n",
      "step 2910 \t loss = 2.219, train_acc = 0.500 (3.251 sec/step)\n",
      "step 2920 \t loss = 2.001, train_acc = 0.500 (3.190 sec/step)\n",
      "step 2930 \t loss = 1.865, train_acc = 0.500 (3.289 sec/step)\n",
      "step 2940 \t loss = 2.518, train_acc = 0.200 (3.198 sec/step)\n",
      "step 2950 \t loss = 2.393, train_acc = 0.500 (3.257 sec/step)\n",
      "step 2960 \t loss = 1.490, train_acc = 0.300 (3.224 sec/step)\n",
      "step 2970 \t loss = 2.305, train_acc = 0.400 (3.184 sec/step)\n",
      "step 2980 \t loss = 2.175, train_acc = 0.400 (3.226 sec/step)\n",
      "step 2990 \t loss = 2.750, train_acc = 0.300 (3.192 sec/step)\n",
      "step 3000 \t loss = 2.671, train_acc = 0.100 (3.172 sec/step)\n",
      "step 3010 \t loss = 2.048, train_acc = 0.500 (3.212 sec/step)\n",
      "step 3020 \t loss = 1.551, train_acc = 0.600 (3.220 sec/step)\n",
      "step 3030 \t loss = 2.339, train_acc = 0.400 (3.240 sec/step)\n",
      "step 3040 \t loss = 2.343, train_acc = 0.200 (3.243 sec/step)\n",
      "step 3050 \t loss = 1.204, train_acc = 0.500 (3.197 sec/step)\n",
      "step 3060 \t loss = 2.140, train_acc = 0.300 (3.189 sec/step)\n",
      "step 3070 \t loss = 2.328, train_acc = 0.100 (3.251 sec/step)\n",
      "step 3080 \t loss = 2.249, train_acc = 0.500 (3.217 sec/step)\n",
      "step 3090 \t loss = 2.343, train_acc = 0.200 (3.192 sec/step)\n",
      "step 3100 \t loss = 1.793, train_acc = 0.300 (3.293 sec/step)\n",
      "step 3110 \t loss = 2.420, train_acc = 0.300 (3.218 sec/step)\n",
      "step 3120 \t loss = 2.058, train_acc = 0.400 (3.254 sec/step)\n",
      "step 3130 \t loss = 1.974, train_acc = 0.400 (3.164 sec/step)\n",
      "step 3140 \t loss = 1.921, train_acc = 0.600 (3.197 sec/step)\n",
      "step 3150 \t loss = 1.572, train_acc = 0.600 (3.176 sec/step)\n",
      "step 3160 \t loss = 1.711, train_acc = 0.400 (3.243 sec/step)\n",
      "step 3170 \t loss = 2.057, train_acc = 0.500 (3.180 sec/step)\n",
      "step 3180 \t loss = 1.421, train_acc = 0.600 (3.250 sec/step)\n",
      "step 3190 \t loss = 2.715, train_acc = 0.400 (3.200 sec/step)\n",
      "step 3200 \t loss = 1.099, train_acc = 0.500 (3.246 sec/step)\n",
      "step 3210 \t loss = 1.907, train_acc = 0.300 (3.199 sec/step)\n",
      "step 3220 \t loss = 1.808, train_acc = 0.500 (3.266 sec/step)\n",
      "step 3230 \t loss = 1.329, train_acc = 0.800 (3.223 sec/step)\n",
      "step 3240 \t loss = 1.427, train_acc = 0.900 (3.285 sec/step)\n",
      "step 3250 \t loss = 3.186, train_acc = 0.200 (3.202 sec/step)\n",
      "step 3260 \t loss = 2.527, train_acc = 0.100 (3.164 sec/step)\n",
      "step 3270 \t loss = 1.808, train_acc = 0.500 (3.261 sec/step)\n",
      "step 3280 \t loss = 2.223, train_acc = 0.200 (3.219 sec/step)\n",
      "step 3290 \t loss = 1.971, train_acc = 0.500 (3.208 sec/step)\n",
      "step 3300 \t loss = 1.763, train_acc = 0.500 (3.222 sec/step)\n",
      "step 3310 \t loss = 2.313, train_acc = 0.300 (3.212 sec/step)\n",
      "step 3320 \t loss = 2.029, train_acc = 0.500 (3.278 sec/step)\n",
      "step 3330 \t loss = 1.683, train_acc = 0.400 (3.258 sec/step)\n",
      "step 3340 \t loss = 1.625, train_acc = 0.500 (3.202 sec/step)\n",
      "step 3350 \t loss = 2.566, train_acc = 0.100 (3.200 sec/step)\n",
      "step 3360 \t loss = 1.621, train_acc = 0.500 (3.171 sec/step)\n",
      "step 3370 \t loss = 2.304, train_acc = 0.300 (3.167 sec/step)\n",
      "step 3380 \t loss = 3.079, train_acc = 0.200 (3.254 sec/step)\n",
      "step 3390 \t loss = 2.216, train_acc = 0.300 (3.184 sec/step)\n",
      "step 3400 \t loss = 2.157, train_acc = 0.300 (3.219 sec/step)\n",
      "step 3410 \t loss = 2.374, train_acc = 0.400 (3.213 sec/step)\n",
      "step 3420 \t loss = 1.994, train_acc = 0.400 (3.199 sec/step)\n",
      "step 3430 \t loss = 1.586, train_acc = 0.400 (3.214 sec/step)\n",
      "step 3440 \t loss = 1.915, train_acc = 0.200 (3.212 sec/step)\n",
      "step 3450 \t loss = 2.021, train_acc = 0.400 (3.203 sec/step)\n",
      "step 3460 \t loss = 1.505, train_acc = 0.500 (3.249 sec/step)\n",
      "step 3470 \t loss = 1.758, train_acc = 0.500 (3.240 sec/step)\n",
      "step 3480 \t loss = 1.587, train_acc = 0.500 (3.250 sec/step)\n",
      "step 3490 \t loss = 1.419, train_acc = 0.700 (3.220 sec/step)\n",
      "step 3500 \t loss = 2.999, train_acc = 0.300 (3.229 sec/step)\n",
      "step 3510 \t loss = 1.914, train_acc = 0.500 (3.200 sec/step)\n",
      "step 3520 \t loss = 1.386, train_acc = 0.700 (3.182 sec/step)\n",
      "step 3530 \t loss = 3.238, train_acc = 0.100 (3.175 sec/step)\n",
      "step 3540 \t loss = 2.657, train_acc = 0.200 (3.222 sec/step)\n",
      "step 3550 \t loss = 2.712, train_acc = 0.100 (3.174 sec/step)\n",
      "step 3560 \t loss = 2.198, train_acc = 0.200 (3.204 sec/step)\n",
      "step 3570 \t loss = 1.839, train_acc = 0.300 (3.242 sec/step)\n",
      "step 3580 \t loss = 1.541, train_acc = 0.600 (3.178 sec/step)\n",
      "step 3590 \t loss = 2.218, train_acc = 0.400 (3.201 sec/step)\n",
      "step 3600 \t loss = 0.933, train_acc = 0.800 (3.193 sec/step)\n",
      "step 3610 \t loss = 2.583, train_acc = 0.300 (3.194 sec/step)\n",
      "step 3620 \t loss = 1.724, train_acc = 0.500 (3.214 sec/step)\n",
      "step 3630 \t loss = 3.358, train_acc = 0.400 (3.194 sec/step)\n",
      "step 3640 \t loss = 2.577, train_acc = 0.400 (3.282 sec/step)\n",
      "step 3650 \t loss = 1.456, train_acc = 0.600 (3.225 sec/step)\n",
      "step 3660 \t loss = 1.879, train_acc = 0.500 (3.234 sec/step)\n",
      "step 3670 \t loss = 0.983, train_acc = 0.600 (3.211 sec/step)\n",
      "step 3680 \t loss = 1.681, train_acc = 0.400 (3.307 sec/step)\n",
      "step 3690 \t loss = 1.894, train_acc = 0.600 (3.275 sec/step)\n",
      "step 3700 \t loss = 2.530, train_acc = 0.100 (3.242 sec/step)\n",
      "step 3710 \t loss = 1.736, train_acc = 0.500 (3.275 sec/step)\n",
      "step 3720 \t loss = 2.382, train_acc = 0.300 (3.226 sec/step)\n",
      "step 3730 \t loss = 2.589, train_acc = 0.300 (3.234 sec/step)\n",
      "step 3740 \t loss = 2.608, train_acc = 0.200 (3.200 sec/step)\n",
      "step 3750 \t loss = 1.472, train_acc = 0.600 (3.184 sec/step)\n",
      "step 3760 \t loss = 2.506, train_acc = 0.400 (3.194 sec/step)\n",
      "step 3770 \t loss = 1.223, train_acc = 0.700 (3.186 sec/step)\n",
      "step 3780 \t loss = 1.326, train_acc = 0.700 (3.203 sec/step)\n",
      "step 3790 \t loss = 2.624, train_acc = 0.300 (3.191 sec/step)\n",
      "VALIDATION \t acc = 0.401 (3.662 sec)\n",
      "New Best Accuracy 0.401 > Old Best 0.273.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 3800 \t loss = 3.335, train_acc = 0.300 (3.176 sec/step)\n",
      "step 3810 \t loss = 2.414, train_acc = 0.300 (3.220 sec/step)\n",
      "step 3820 \t loss = 1.159, train_acc = 0.500 (3.246 sec/step)\n",
      "step 3830 \t loss = 2.230, train_acc = 0.200 (3.244 sec/step)\n",
      "step 3840 \t loss = 1.158, train_acc = 0.600 (3.259 sec/step)\n",
      "step 3850 \t loss = 2.065, train_acc = 0.200 (3.268 sec/step)\n",
      "step 3860 \t loss = 0.917, train_acc = 0.700 (3.330 sec/step)\n",
      "step 3870 \t loss = 1.678, train_acc = 0.400 (3.242 sec/step)\n",
      "step 3880 \t loss = 2.507, train_acc = 0.300 (3.186 sec/step)\n",
      "step 3890 \t loss = 1.585, train_acc = 0.600 (3.172 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3900 \t loss = 1.573, train_acc = 0.400 (3.186 sec/step)\n",
      "step 3910 \t loss = 1.900, train_acc = 0.300 (3.202 sec/step)\n",
      "step 3920 \t loss = 1.980, train_acc = 0.500 (3.281 sec/step)\n",
      "step 3930 \t loss = 1.156, train_acc = 0.600 (3.214 sec/step)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model Setup\n",
    "'''\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(BATCH_SIZE, 32, 32, 3))\n",
    "x_resized = tf.image.resize_images(x, (224, 224))\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(BATCH_SIZE))\n",
    "is_training = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "net = vgg16(x_resized, y, is_training)\n",
    "\n",
    "'''\n",
    "Loss, Metrics, and Optimization Setup\n",
    "'''\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, #GT probability distribution\n",
    "        logits=net.fc3, # unscaled log prob\n",
    "        name='sparse_softmax_cross_entropy')\n",
    "\n",
    "reduced_loss = tf.reduce_mean(loss)\n",
    "train_loss_summary = tf.summary.scalar('training_loss', reduced_loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        beta1=BETA1,\n",
    "        beta2=BETA2,\n",
    "        name='AdamOptimizer')\n",
    "train_op = optimizer.minimize(reduced_loss)\n",
    "\n",
    "pred = tf.nn.softmax(\n",
    "        logits=net.fc3,\n",
    "        name='softmax')\n",
    "pred_class = tf.cast(tf.argmax(pred, axis=1), tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(\n",
    "        tf.equal(y, pred_class),\n",
    "        tf.float32))\n",
    "\n",
    "train_acc_summary = tf.summary.scalar('training_accuracy', acc)\n",
    "\n",
    "\n",
    "'''\n",
    "TensorBoard Setup\n",
    "'''\n",
    "all_train_summary = tf.summary.merge_all()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(SNAPSHOT_DIR,\n",
    "        graph=tf.get_default_graph())\n",
    "\n",
    "'''\n",
    "Tensorflow Saver Setup\n",
    "'''\n",
    "saver = tf.train.Saver(var_list=tf.global_variables(),\n",
    "                       max_to_keep=SNAPSHOT_MAX)\n",
    "\n",
    "'''\n",
    "Tensorflow Session Setup\n",
    "'''\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "init = tf.group(tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer())\n",
    "sess.run(init)\n",
    "\n",
    "'''\n",
    "Load Pretrained Weights (ImageNet)\n",
    "'''\n",
    "net.load_pretrained_weights(sess)\n",
    "\n",
    "'''\n",
    "Declare Validation Loop\n",
    "'''\n",
    "def run_validation():\n",
    "    global best_acc\n",
    "    \n",
    "    start_t = time()\n",
    "    overall_acc = 0\n",
    "    overall_loss = 0\n",
    "    for j in range(0, n_validation, BATCH_SIZE):\n",
    "        # Assemble Batch\n",
    "        idx = validation_indices[j:(j+BATCH_SIZE)]\n",
    "        x_batch = data_x[idx,...]\n",
    "        y_batch = data_y[idx,...]\n",
    "        \n",
    "        feed_dict = {x:x_batch,\n",
    "                 y:y_batch,\n",
    "                 is_training: False}\n",
    "        loss_v, acc_v, pred_v = sess.run(\n",
    "                [reduced_loss, acc, pred],\n",
    "                feed_dict=feed_dict)\n",
    "        overall_acc += acc_v\n",
    "        overall_loss += loss_v\n",
    "        \n",
    "        \n",
    "    duration = time() - start_t\n",
    "    overall_acc /= (n_validation / BATCH_SIZE)\n",
    "    overall_loss /= (n_validation / BATCH_SIZE)\n",
    "    \n",
    "    overall_acc_summary = tf.Summary()\n",
    "    overall_acc_summary.value.add(tag='validation_accuracy', simple_value=overall_acc)\n",
    "    overall_loss_summary = tf.Summary()\n",
    "    overall_loss_summary.value.add(tag='validation_loss', simple_value=overall_loss)\n",
    "\n",
    "    summary_writer.add_summary(overall_acc_summary, step)\n",
    "    summary_writer.add_summary(overall_loss_summary, step)\n",
    "    \n",
    "    print('VALIDATION \\t acc = {:.3f} ({:.3f} sec)'.format(\n",
    "                overall_acc, duration))\n",
    "    if overall_acc > best_acc:\n",
    "        print('New Best Accuracy {:.3f} > Old Best {:.3f}.  Saving...'.format(\n",
    "                overall_acc, best_acc))\n",
    "        best_acc = overall_acc\n",
    "        save(saver, sess, SNAPSHOT_DIR, step)\n",
    "        \n",
    "'''\n",
    "Main Training Loop\n",
    "'''\n",
    "step = 0\n",
    "epoch = 0\n",
    "best_acc = 0\n",
    "while epoch < NUM_EPOCH:\n",
    "    step += 1\n",
    "    # Allocate Space For Batch\n",
    "    x_batch = np.zeros((BATCH_SIZE,) + data_x.shape[1:], dtype=np.float32)\n",
    "    y_batch = np.zeros((BATCH_SIZE,) + data_y.shape[1:], dtype=np.int32)\n",
    "    \n",
    "    # Run Validation\n",
    "    if step % batches_per_epoch == 0:\n",
    "        epoch += 1\n",
    "        run_validation()\n",
    "        \n",
    "    # Form Training Batch\n",
    "    start_t = time()\n",
    "    for i in range(BATCH_SIZE):\n",
    "        idx = next(train_indices)\n",
    "        x_batch[i,...] = data_x[idx, ...]\n",
    "        y_batch[i,...] = data_y[idx, ...]\n",
    "    \n",
    "    # Data Augmentation\n",
    "    if random.random() < 0.5:\n",
    "        x_batch = np.fliplr(x_batch)\n",
    "        \n",
    "    # Prepare Feed Dictionary\n",
    "    feed_dict = {x:x_batch,\n",
    "                 y:y_batch,\n",
    "                 is_training: True}\n",
    "    # Run Training Summary\n",
    "    if step % SUMMARY_EVERY == 0:\n",
    "        loss_v, _, summary_v, acc_v, pred_v = sess.run(\n",
    "                [reduced_loss, train_op, all_train_summary, acc, pred],\n",
    "                feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_v, step)\n",
    "        duration = time() - start_t\n",
    "        print('step {:d} \\t loss = {:.3f}, train_acc = {:.3f} ({:.3f} sec/step)'.format(\n",
    "                step, loss_v, acc_v, duration))\n",
    "    else: # Run Simple Train\n",
    "        loss_v, _ = sess.run([reduced_loss, train_op],\n",
    "                feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
