{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'experiment_01_stanford40_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import cycle\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from utils import optimistic_restore, save, load\n",
    "import layers\n",
    "\n",
    "PWD = os.getcwd()\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(PWD, '..')))\n",
    "import pegasos\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "HYPERPARAMS\n",
    "'''\n",
    "TRAIN = False\n",
    "BATCH_SIZE = 10\n",
    "PATIENCE = 2\n",
    "TRIPLETS_TRAIN = '/media/red/capstone/data/stanford40_triplets_train.pkl'\n",
    "TRIPLETS_VALIDATION = '/media/red/capstone/data/stanford40_triplets_val.pkl'\n",
    "FEATURE_FILE = '/media/red/capstone/data/stanford_40/vgg16_features.pkl'\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "LEARNING_RATE_DECAY = 0.7\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.99\n",
    "NUM_EPOCH = 100\n",
    "RANDOM_SEED = 1234\n",
    "SUMMARY_EVERY = 10\n",
    "VALIDATION_PERCENTAGE = 0.05\n",
    "SNAPSHOT_MAX = 3 # Keeps the last best 10 snapshots (best determined by validation accuracy)\n",
    "SNAPSHOT_DIR = os.path.join('/media/red/capstone/snapshots/', EXPERIMENT_NAME)\n",
    "RESTORE_FROM = os.path.join(SNAPSHOT_DIR, 'model.ckpt-14832')\n",
    "\n",
    "\n",
    "# Network params\n",
    "NORMALIZE = True\n",
    "N_FEAT = 4096\n",
    "\n",
    "np.random.seed(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load Triplets\n",
    "'''\n",
    "class SVM_Triplet:\n",
    "    def __init__(self, X1, X2, Y, base_classes, pos_class, new_class):\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.Y = Y\n",
    "        self.base_classes = base_classes\n",
    "        self.pos_class = pos_class\n",
    "        self.new_class = new_class\n",
    "        \n",
    "# Load features\n",
    "triplets_loadin = {}\n",
    "triplets_loadin['train'] = pickle.load(open(TRIPLETS_TRAIN, \"rb\"))\n",
    "triplets_loadin['validation'] = pickle.load(open(TRIPLETS_VALIDATION, \"rb\"))\n",
    "\n",
    "x_data = {\n",
    "    'train':[],\n",
    "    'validation':[]\n",
    "}\n",
    "y_data = {\n",
    "    'train':[],\n",
    "    'validation':[]\n",
    "}\n",
    "for partition in ['train', 'validation']:\n",
    "    for triplet in triplets_loadin[partition]:\n",
    "        if NORMALIZE:\n",
    "            X1 = triplet.X1 / np.linalg.norm(triplet.X1, axis=0, keepdims=True)\n",
    "            X2 = triplet.X2 / np.linalg.norm(triplet.X2, axis=0, keepdims=True)\n",
    "            Y  = triplet.Y  / np.linalg.norm(triplet.Y,  axis=0, keepdims=True)\n",
    "            x_data[partition].append(np.hstack((X1, X2)))\n",
    "            y_data[partition].append(Y-X1)\n",
    "        else:   \n",
    "            x_data[partition].append(np.hstack((triplet.X1, triplet.X2)))\n",
    "            y_data[partition].append(triplet.Y-triplet.X1)\n",
    "        \n",
    "x_data['train'] = np.stack(x_data['train'])\n",
    "y_data['train'] = np.stack(y_data['train'])\n",
    "x_data['validation'] = np.stack(x_data['validation'])\n",
    "y_data['validation'] = np.stack(y_data['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Declare model\n",
    "'''\n",
    "\n",
    "def lrelu(x, alpha=0.1):\n",
    "    return tf.nn.relu(x) - alpha * tf.nn.relu(-x)\n",
    "\n",
    "def net(x, is_training):\n",
    "    def dense_block(n_units):\n",
    "        stack.append(layers.fc(\n",
    "            input=stack[-1],\n",
    "            units=n_units,\n",
    "            activation='relu',\n",
    "            name='fc'\n",
    "            )[0])\n",
    "        stack.append(tf.contrib.layers.batch_norm(\n",
    "                stack[-1], \n",
    "                center=True, scale=True, \n",
    "                is_training=is_training,\n",
    "                scope='bn'))\n",
    "        stack.append(lrelu(stack[-1]))\n",
    "        \n",
    "    n_units_list = [2*N_FEAT, 2*N_FEAT, N_FEAT]\n",
    "    stack = [x,]\n",
    "    for i, n in enumerate(n_units_list):\n",
    "        with tf.variable_scope(\"block_\"+str(i)):\n",
    "            dense_block(n)\n",
    "    stack.append(layers.fc(\n",
    "            input=stack[-1],\n",
    "            units=4096,\n",
    "            activation='linear',\n",
    "            name='fc_final'\n",
    "            )[0])\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Weights\n",
      "INFO:tensorflow:Restoring parameters from /media/red/capstone/snapshots/experiment_01_stanford40_train/model.ckpt-14832\n",
      "Restored model parameters from /media/red/capstone/snapshots/experiment_01_stanford40_train/model.ckpt-14832\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model Setup\n",
    "'''\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(None, 8192))\n",
    "if TRAIN:\n",
    "    y = tf.placeholder(dtype=tf.float32, shape=(None, 4096))\n",
    "is_training = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "global_step = tf.get_variable('global_step', dtype=tf.int32, trainable=False, initializer=0) #incremented everytime optimizer runs\n",
    "lr = tf.get_variable('learning_rate', dtype=tf.float32, trainable=False, initializer=LEARNING_RATE)\n",
    "\n",
    "net = net(x, is_training)\n",
    "\n",
    "'''\n",
    "Loss, Metrics, and Optimization Setup\n",
    "'''\n",
    "pred = net[-1]\n",
    "pred_normalized = pred / tf.norm(pred, axis=1, keep_dims=True)\n",
    "if TRAIN:\n",
    "    y_normalized = y / tf.norm(y,axis=1,keep_dims=True)\n",
    "    reduced_loss = tf.losses.cosine_distance(\n",
    "            labels=y_normalized,\n",
    "            predictions=pred_normalized,\n",
    "            dim=1,\n",
    "            reduction=tf.losses.Reduction.MEAN,\n",
    "            )\n",
    "    train_loss_summary = tf.summary.scalar('training_loss', reduced_loss)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=lr,\n",
    "            beta1=BETA1,\n",
    "            beta2=BETA2,\n",
    "            name='AdamOptimizer')\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(reduced_loss, tvars), 5.0)\n",
    "    train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n",
    "\n",
    "'''\n",
    "TensorBoard Setup\n",
    "'''\n",
    "all_train_summary = tf.summary.merge_all()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(SNAPSHOT_DIR,\n",
    "        graph=tf.get_default_graph())\n",
    "\n",
    "'''\n",
    "Tensorflow Saver Setup\n",
    "'''\n",
    "saver = tf.train.Saver(var_list=tf.global_variables(),\n",
    "                       max_to_keep=SNAPSHOT_MAX)\n",
    "\n",
    "'''\n",
    "Tensorflow Session Setup\n",
    "'''\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "init = tf.group(tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer())\n",
    "sess.run(init)\n",
    "\n",
    "'''\n",
    "Restore Weights\n",
    "'''\n",
    "if RESTORE_FROM is not None:\n",
    "    print('Loading Weights')\n",
    "    loader = tf.train.Saver(var_list=tf.global_variables())\n",
    "    load(loader, sess, RESTORE_FROM)\n",
    "else:\n",
    "    pass\n",
    "'''\n",
    "Primary Loop\n",
    "'''\n",
    "if TRAIN:\n",
    "    partition_types = ['train', 'validation']\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    step_v = global_step.eval(session=sess)\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        overall_loss = 0 # Variables used for validation\n",
    "\n",
    "        print('Training Epoch {}/{}'.format(\n",
    "                epoch, NUM_EPOCH))\n",
    "\n",
    "        for partition in partition_types: # Itr through data partitions\n",
    "            n_exemplars = x_data[partition].shape[0]\n",
    "            if partition == 'train':\n",
    "                shuffle_indices = np.arange(n_exemplars)\n",
    "                np.random.shuffle(shuffle_indices)\n",
    "                x_data['train'] = x_data['train'][shuffle_indices, ...]\n",
    "                y_data['train'] = y_data['train'][shuffle_indices, ...]\n",
    "            step_v = global_step.eval(session=sess)\n",
    "            for i in range(0, n_exemplars, BATCH_SIZE):\n",
    "                upper_range = i+BATCH_SIZE\n",
    "                if upper_range > n_exemplars:\n",
    "                    upper_range = n_exemplars\n",
    "                x_batch = x_data[partition][i:upper_range, ...]\n",
    "                y_batch = y_data[partition][i:upper_range, ...]\n",
    "\n",
    "                feed_dict = {\n",
    "                    x:x_batch,\n",
    "                    y:y_batch\n",
    "                }\n",
    "                if partition == 'train':\n",
    "                    feed_dict[is_training] = True\n",
    "                else:\n",
    "                    feed_dict[is_training] = False\n",
    "\n",
    "                # Run the proper sess run command\n",
    "                if partition == 'train':\n",
    "                    start_t = time()\n",
    "                    if step_v % SUMMARY_EVERY == 0:\n",
    "                        _, loss_v, summary_v = sess.run(\n",
    "                            [train_op, reduced_loss, all_train_summary],\n",
    "                            feed_dict=feed_dict)\n",
    "                        summary_writer.add_summary(summary_v, step_v)\n",
    "                        duration = time() - start_t\n",
    "                        print('step {:d} \\t loss = {:.3f} ({:.3f} sec/step)'.format(\n",
    "                                step_v, loss_v, duration))\n",
    "                    else: # Vanilla Training\n",
    "                        _ = sess.run([train_op], feed_dict=feed_dict)\n",
    "                    step_v = global_step.eval(session=sess)\n",
    "                elif partition == 'validation':\n",
    "                    feed_dict[is_training] = False\n",
    "                    loss_v = sess.run(\n",
    "                            [reduced_loss],\n",
    "                            feed_dict=feed_dict)[0]\n",
    "                    overall_loss += loss_v\n",
    "            # Post-epoch routine for validation set (saving, stat computation, etc)\n",
    "            if partition == 'validation':\n",
    "                duration = time() - start_t\n",
    "                overall_loss /= x_data['validation'].shape[0]\n",
    "                overall_loss_summary = tf.Summary()\n",
    "                overall_loss_summary.value.add(tag='validation_loss', simple_value=overall_loss)\n",
    "                summary_writer.add_summary(overall_loss_summary, step_v)\n",
    "\n",
    "                if overall_loss < best_loss:\n",
    "                    print('New Best Loss {:.3f} < Old Best {:.3f}.  Saving...'.format(\n",
    "                            overall_loss, best_loss))\n",
    "                    best_loss = overall_loss\n",
    "                    patience_counter = 0\n",
    "                    save(saver, sess, SNAPSHOT_DIR, step_v)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                patience_counter = 0\n",
    "                lr_v = lr.eval(session=sess) * LEARNING_RATE_DECAY\n",
    "                lr.assign(lr_v).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Accuracy Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pr(y_test, y_gt):\n",
    "    average_precision = average_precision_score(y_test, y_gt)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_gt)\n",
    "\n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.5, color='b')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve: AUC={0:0.2f}'.format(\n",
    "              average_precision))\n",
    "    plt.show()\n",
    "    \n",
    "class SVM_Triplet:\n",
    "    def __init__(self, X1, X2, Y, base_classes, pos_class, new_class):\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.Y = Y\n",
    "        self.base_classes = base_classes\n",
    "        self.pos_class = pos_class\n",
    "        self.new_class = new_class\n",
    "\n",
    "def prepare_features(pos_class, neg_classes, feature_vectors, is_train=True, \n",
    "                     equal_features=False, train_split=0.9):\n",
    "    '''\n",
    "    Returns 4096-dim features for each image which would be used for SVM training\n",
    "    inputs : \n",
    "        is_train -> will return 90% of features for each class if is_train=True, else returns 10% features\n",
    "        equal_features -> if set to True, then len(neg_features) = len(pos_features)\n",
    "    \n",
    "    Returns:\n",
    "        pos_features -> features of images in the positive class\n",
    "        neg_features -> features of images in the negative classes\n",
    "    '''\n",
    "    \n",
    "    pos_partition = int(train_split*feature_vectors[pos_class].shape[0])\n",
    "    neg_features = []\n",
    "    if is_train:\n",
    "        pos_features = feature_vectors[pos_class][:pos_partition]    # n x 4096\n",
    "        for neg_class in neg_classes:\n",
    "            neg_partition = int(train_split*feature_vectors[neg_class].shape[0])\n",
    "            neg_features.extend(feature_vectors[neg_class][:neg_partition])\n",
    "            \n",
    "    else:\n",
    "        pos_features = feature_vectors[pos_class][pos_partition:]    # n x 4096\n",
    "        for neg_class in neg_classes:\n",
    "            neg_partition = int(train_split*feature_vectors[neg_class].shape[0])\n",
    "            neg_features.extend(feature_vectors[neg_class][neg_partition:])\n",
    "             \n",
    "    if equal_features:\n",
    "        neg_features = np.random.permutation(neg_features)[:pos_features.shape[0]]\n",
    "    \n",
    "    return pos_features, neg_features\n",
    "    \n",
    "def compute_accuracy(weight_vector, pos_features, neg_features):\n",
    "    classifier = pegasos.PegasosSVMClassifier()\n",
    "    classifier.fit(np.zeros((2, 1024)), np.asarray([1, 0]))\n",
    "    classifier.weight_vector.weights = weight_vector\n",
    "\n",
    "    # Concat data and pass to SVM\n",
    "    result = classifier.predict(np.vstack((pos_features, neg_features)))\n",
    "    ground_truth = np.concatenate((np.ones(len(pos_features)), np.zeros(len(neg_features))))\n",
    "    return np.average(np.equal(ground_truth, result)), result, ground_truth\n",
    "\n",
    "def get_svm_weights(x_train, y_train, sklearn_SGD=False):\n",
    "    if sklearn_SGD:\n",
    "        clf = linear_model.SGDClassifier()\n",
    "        clf.partial_fit(x_train, y_train, classes=np.unique(y_train))\n",
    "        weights = clf.coef_\n",
    "        return clf\n",
    "    else:\n",
    "        svm = pegasos.PegasosSVMClassifier()\n",
    "        svm.fit(x_train, y_train)\n",
    "        weight_vector = svm.weight_vector.weights\n",
    "        return weight_vector\n",
    "\n",
    "\n",
    "def get_x_y(pos_features, neg_features):\n",
    "    x = np.vstack((pos_features, neg_features))\n",
    "    y = np.hstack((np.ones( len(pos_features)),\n",
    "                   np.zeros(len(neg_features))))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "'''\n",
    "We only need the negative features for the novel class. \n",
    "(It is negative with respect to the positive class)\n",
    "'''\n",
    "def online_svm_update(clf, neg_features):\n",
    "    clf.partial_fit(neg_features, np.zeros(len(neg_features)))\n",
    "    return clf\n",
    "    \n",
    "\n",
    "'''\n",
    "SVM for novel class. \n",
    "pos_class = pos_class\n",
    "neg_classes = base_classes - pos_class\n",
    "'''\n",
    "def compute_X1(pos_class, base_classes, feature_vectors, is_train=True, sklearn_SGD=False):\n",
    "    neg_classes = np.delete(base_classes, np.argwhere(base_classes==pos_class))\n",
    "    pos_features, neg_features = prepare_features(pos_class, neg_classes, feature_vectors, is_train=is_train)\n",
    "    x_train, y_train = get_x_y(pos_features, neg_features)\n",
    "    return get_svm_weights(x_train, y_train, sklearn_SGD=sklearn_SGD)\n",
    "    \n",
    "\n",
    "\n",
    "'''\n",
    "SVM for novel class. \n",
    "pos_class = novel_class\n",
    "neg_classes = base_classes\n",
    "'''\n",
    "def compute_X2(novel_class, base_classes, feature_vectors, is_train=True, sklearn_SGD=False):\n",
    "    pos_features, neg_features = prepare_features(novel_class, base_classes, feature_vectors, is_train=is_train)\n",
    "    x_train, y_train = get_x_y(pos_features, neg_features)\n",
    "    return get_svm_weights(x_train, y_train, sklearn_SGD=sklearn_SGD)\n",
    "    \n",
    "'''\n",
    "SVM for pos class under the influence of the neg class. \n",
    "pos_class = pos_class\n",
    "neg_classes = base_classes - pos_class + novel_class\n",
    "'''\n",
    "def compute_Y(pos_class, novel_class, base_classes, feature_vectors, is_train=True, sklearn_SGD=False):\n",
    "    neg_classes = np.delete(base_classes, np.argwhere(base_classes==pos_class))\n",
    "    neg_classes = np.append(neg_classes, novel_class)\n",
    "    pos_features, neg_features = prepare_features(pos_class, neg_classes, feature_vectors, is_train=is_train)\n",
    "    x_train, y_train = get_x_y(pos_features, neg_features)\n",
    "    return get_svm_weights(x_train, y_train, sklearn_SGD=sklearn_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88060 0.88060 0.00000 jumping|cleaning_the_floor\n",
      "0.87970 0.87970 0.00000 jumping|brushing_teeth\n",
      "0.86517 0.86517 0.00000 jumping|cutting_trees\n",
      "0.88364 0.88364 0.00000 jumping|cooking\n",
      "0.82609 0.82609 0.00000 jumping|climbing\n",
      "0.87454 0.87454 0.00000 jumping|reading\n",
      "0.87868 0.87868 0.00000 jumping|drinking\n",
      "0.87925 0.87925 0.00000 jumping|washing_dishes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7fe96f531069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Test out our incremental hypothesis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_X2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mX2_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d13d549f9443>\u001b[0m in \u001b[0;36mcompute_X2\u001b[0;34m(novel_class, base_classes, feature_vectors, is_train, sklearn_SGD)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mpos_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnovel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_x_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_svm_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msklearn_SGD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msklearn_SGD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m '''\n",
      "\u001b[0;32m<ipython-input-7-d13d549f9443>\u001b[0m in \u001b[0;36mget_svm_weights\u001b[0;34m(x_train, y_train, sklearn_SGD)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0msvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpegasos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPegasosSVMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mweight_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mweight_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incremental-learning/pegasos/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mpegasos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_stochastic_balanced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOOP_STOCHASTIC\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mpegasos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_stochastic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s: unknown loop type'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incremental-learning/pegasos/pegasos.py\u001b[0m in \u001b[0;36mtrain_stochastic\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNER_PEGASOS_SVM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0m_single_svm_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNER_PEGASOS_LOGREG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0m_single_logreg_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incremental-learning/pegasos/pegasos.py\u001b[0m in \u001b[0;36m_single_svm_step\u001b[0;34m(xi, yi, w, eta, lambda_reg)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_single_svm_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mL2_regularize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0myi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incremental-learning/pegasos/pegasos.py\u001b[0m in \u001b[0;36mL2_regularize\u001b[0;34m(w, eta, lambda_reg)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mL2_regularize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mscaling_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlambda_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaling_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMIN_SCALING_FACTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0metaval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incremental-learning/pegasos/weight_vector.py\u001b[0m in \u001b[0;36mscale_to\u001b[0;34m(self, scaling_factor)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscale_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMIN_SCALE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compare our model with the baseline\n",
    "features, file_names = pickle.load(open(FEATURE_FILE, \"rb\"))\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Splitting classes into train/val/test\n",
    "labels = np.array(list(features.keys()))\n",
    "perm = np.random.permutation(len(labels))\n",
    "labels = labels[perm]\n",
    "\n",
    "splits = {}\n",
    "splits['base']=labels[:15]\n",
    "splits['novel'] = labels[15:25]\n",
    "splits['test']=labels[25:]\n",
    "\n",
    "acc1_all, acc2_all = [], []\n",
    "\n",
    "base_class_ind = np.random.permutation(len(splits['base']))[:10]\n",
    "base_classes = splits['base'][base_class_ind]\n",
    "  \n",
    "for pos_class in base_classes:\n",
    "    acc1, acc2 = [], []\n",
    "    X1 = compute_X1(pos_class, base_classes, features, is_train=True)\n",
    "    X1_norm = X1 / np.linalg.norm(X1, axis=0, keepdims=True)\n",
    "    neg_classes = np.delete(base_classes, np.argwhere(base_classes==pos_class))\n",
    "    \n",
    "    for new_class in splits['test']:\n",
    "        neg_classes_extra = np.append(neg_classes, new_class)    # 9 + 1 classes\n",
    "        pos_features_test, neg_features_test = prepare_features(pos_class, \n",
    "                                                                neg_classes_extra, \n",
    "                                                                features, \n",
    "                                                                is_train=False, \n",
    "                                                                equal_features=False)\n",
    "        acc1.append(compute_accuracy(X1, pos_features_test, neg_features_test)[0])\n",
    "\n",
    "        # Test out our incremental hypothesis\n",
    "        X2 = compute_X2(new_class, base_classes, features, is_train=True)\n",
    "        X2_norm = X2 / np.linalg.norm(X2, axis=0, keepdims=True)\n",
    "        X = np.hstack((X1_norm, X2_norm))\n",
    "        X = np.reshape(X, (1, 8192))\n",
    "        feed_dict = {\n",
    "            x:X,\n",
    "            is_training: False\n",
    "        }\n",
    "        Y_hat = sess.run(\n",
    "                [pred],\n",
    "                feed_dict=feed_dict)[0]\n",
    "        Y = X1 + Y_hat.reshape((4096))\n",
    "        acc2.append(compute_accuracy(Y, pos_features_test, neg_features_test)[0])\n",
    "        #pdb.set_trace()\n",
    "        print('%.5f %.5f %.5f %s|%s' % (acc1[-1], acc2[-1], acc2[-1] - acc1[-1], pos_class, new_class)) \n",
    "\n",
    "    acc1_all.append(acc1)\n",
    "    acc2_all.append(acc2)\n",
    "    \n",
    "    \n",
    "acc1_all = np.array(acc1_all)\n",
    "acc2_all = np.array(acc2_all)\n",
    "acc1_mean = np.mean(acc1_all, axis=0)\n",
    "acc2_mean = np.mean(acc2_all, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
