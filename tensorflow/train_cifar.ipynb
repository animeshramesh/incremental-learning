{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo:\n",
    "- Experiment with normalization while creating triplets\n",
    "- Do you need dropout in the regressor network?\n",
    "- Simultaneous feature learning?\n",
    "- See when are results bad/equal/better? See which classes they correspond to.. \n",
    "\n",
    "# Todo on TF model\n",
    "- implement data import\n",
    "- implement data normalization\n",
    "- debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from utils import optimistic_restore, save\n",
    "import layers\n",
    "\n",
    "PWD = os.getcwd()\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(PWD, '..')))\n",
    "import pickle_utils\n",
    "import cifar_utils\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "HYPERPARAMS\n",
    "'''\n",
    "BATCH_SIZE = 10\n",
    "DATA_PATH = '/media/red/capstone/data/cifar-100/cifar-custom'\n",
    "LEARNING_RATE = 2.5e-4\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.99\n",
    "NUM_CLASSES = 40\n",
    "NUM_EPOCH = 100\n",
    "RANDOM_SEED = 1234\n",
    "SUMMARY_EVERY = 1000\n",
    "VALIDATION_PERCENTAGE = 0.05\n",
    "SNAPSHOT_MAX = 10 # Keeps the last best 10 snapshots (best determined by validation accuracy)\n",
    "SNAPSHOT_DIR = '/media/red/capstone/snapshots/feature_extractor_cnn'\n",
    "\n",
    "np.random.seed(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load custom CIFAR data. \n",
    "'''\n",
    "# cifar_raw = pickle_utils.load(DATA_PATH)\n",
    "custom_dataset = pickle_utils.load(DATA_PATH)\n",
    "\n",
    "data_x, data_y = [], []\n",
    "for label in custom_dataset['training'].keys():\n",
    "    for item in custom_dataset['training'][label]:\n",
    "        data_x.append(item) # 28 x 28 x 3\n",
    "        data_y.append(label) # 0-39\n",
    "data_x = np.stack(data_x).astype(np.float32)\n",
    "data_y = np.stack(data_y).astype(np.int32)\n",
    "\n",
    "# Normalize x\n",
    "data_x = (data_x / 255.0) - 0.5\n",
    "\n",
    "def round_to(n, precision):\n",
    "    return int( n/precision+0.5 ) * precision\n",
    "\n",
    "n_total_data = data_x.shape[0]\n",
    "n_validation = round_to(VALIDATION_PERCENTAGE * n_total_data, BATCH_SIZE)\n",
    "batches_per_epoch = np.round((n_total_data - n_validation) / BATCH_SIZE)\n",
    "# Shuffle data\n",
    "random_indices = np.random.permutation(n_total_data)\n",
    "train_indices = cycle(random_indices[n_validation:])\n",
    "validation_indices = random_indices[:n_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Declare model\n",
    "'''\n",
    "def feature_extractor_cnn(x, y, is_training):\n",
    "    net = []\n",
    "    net.append(layers.conv2d(\n",
    "            input=x,\n",
    "            shape=(3,3,3,32),\n",
    "            padding='SAME',\n",
    "            activation='relu',\n",
    "            name='conv_1_1'\n",
    "            )[0])\n",
    "    net.append(layers.batch_norm(\n",
    "            x=net[-1],\n",
    "            phase=is_training,\n",
    "            name='bn_1_2'))\n",
    "    net.append(layers.conv2d(\n",
    "            input=net[-1],\n",
    "            shape=(3,3,32,32),\n",
    "            padding='SAME',\n",
    "            activation='relu',\n",
    "            bias=False,\n",
    "            name='conv_1_2'\n",
    "            )[0])\n",
    "    net.append(tf.nn.max_pool(\n",
    "            value=net[-1],\n",
    "            ksize=[1, 2, 2, 1],\n",
    "            strides=[1,2,2,1],\n",
    "            padding='SAME',\n",
    "            name='pool_1'\n",
    "            ))\n",
    "    net.append(layers.batch_norm(\n",
    "            x=net[-1],\n",
    "            phase=is_training,\n",
    "            name='bn_2_1'))\n",
    "    net.append(layers.conv2d(\n",
    "            input=net[-1],\n",
    "            shape=(3,3,32,64),\n",
    "            padding='SAME',\n",
    "            activation='relu',\n",
    "            bias=False,\n",
    "            name='conv_2_1'\n",
    "            )[0])\n",
    "    net.append(layers.batch_norm(\n",
    "            x=net[-1],\n",
    "            phase=is_training,\n",
    "            name='bn_2_2'))\n",
    "    net.append(layers.conv2d(\n",
    "            input=net[-1],\n",
    "            shape=(3,3,64,64),\n",
    "            padding='SAME',\n",
    "            activation='relu',\n",
    "            bias=False,\n",
    "            name='conv_2_2'\n",
    "            )[0])\n",
    "    net.append(tf.nn.max_pool(\n",
    "            value=net[-1],\n",
    "            ksize=[1, 2, 2, 1],\n",
    "            strides=[1,2,2,1],\n",
    "            padding='SAME',\n",
    "            name='pool_2'\n",
    "            ))\n",
    "\n",
    "    net.append(tf.contrib.layers.flatten(\n",
    "            inputs=net[-1],\n",
    "            scope='flat_1'\n",
    "            ))\n",
    "    net.append(layers.fc(\n",
    "            input=net[-1],\n",
    "            units=1024,\n",
    "            activation='relu',\n",
    "            name='fc_1'\n",
    "            )[0])\n",
    "    net.append(tf.nn.dropout(\n",
    "            x=net[-1],\n",
    "            keep_prob=0.5,\n",
    "            name='dropout_fc_1'\n",
    "            ))\n",
    "    net.append(layers.fc(\n",
    "            input=net[-1],\n",
    "            units=1024,\n",
    "            activation='relu',\n",
    "            name='fc_2'\n",
    "            )[0])\n",
    "    net.append(tf.nn.dropout(\n",
    "            x=net[-1],\n",
    "            keep_prob=0.5,\n",
    "            name='dropout_fc_2'\n",
    "            ))\n",
    "    net.append(layers.fc(\n",
    "            input=net[-1],\n",
    "            units=NUM_CLASSES,\n",
    "            activation='linear',\n",
    "            name='fc_3'\n",
    "            )[0])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000 \t loss = 2.602, train_acc = 0.300 (0.174 sec/step)\n",
      "VALIDATION \t acc = 0.146 (0.375 sec)\n",
      "New Best Accuracy 0.146 > Old Best 0.000.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 2000 \t loss = 3.471, train_acc = 0.000 (0.141 sec/step)\n",
      "step 3000 \t loss = 3.241, train_acc = 0.000 (0.127 sec/step)\n",
      "VALIDATION \t acc = 0.180 (0.358 sec)\n",
      "New Best Accuracy 0.180 > Old Best 0.146.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 4000 \t loss = 3.197, train_acc = 0.200 (0.129 sec/step)\n",
      "step 5000 \t loss = 2.443, train_acc = 0.200 (0.127 sec/step)\n",
      "VALIDATION \t acc = 0.256 (0.347 sec)\n",
      "New Best Accuracy 0.256 > Old Best 0.180.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 6000 \t loss = 3.600, train_acc = 0.100 (0.135 sec/step)\n",
      "step 7000 \t loss = 1.493, train_acc = 0.700 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.324 (0.343 sec)\n",
      "New Best Accuracy 0.324 > Old Best 0.256.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 8000 \t loss = 2.797, train_acc = 0.200 (0.130 sec/step)\n",
      "step 9000 \t loss = 2.374, train_acc = 0.200 (0.130 sec/step)\n",
      "VALIDATION \t acc = 0.352 (0.344 sec)\n",
      "New Best Accuracy 0.352 > Old Best 0.324.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 10000 \t loss = 2.402, train_acc = 0.400 (0.129 sec/step)\n",
      "step 11000 \t loss = 2.078, train_acc = 0.500 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.378 (0.350 sec)\n",
      "New Best Accuracy 0.378 > Old Best 0.352.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 12000 \t loss = 2.595, train_acc = 0.300 (0.129 sec/step)\n",
      "step 13000 \t loss = 2.544, train_acc = 0.300 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.389 (0.350 sec)\n",
      "New Best Accuracy 0.389 > Old Best 0.378.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 14000 \t loss = 3.283, train_acc = 0.100 (0.132 sec/step)\n",
      "step 15000 \t loss = 0.323, train_acc = 1.000 (0.131 sec/step)\n",
      "VALIDATION \t acc = 0.407 (0.360 sec)\n",
      "New Best Accuracy 0.407 > Old Best 0.389.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 16000 \t loss = 1.762, train_acc = 0.400 (0.132 sec/step)\n",
      "step 17000 \t loss = 1.130, train_acc = 0.700 (0.131 sec/step)\n",
      "VALIDATION \t acc = 0.381 (0.343 sec)\n",
      "step 18000 \t loss = 0.982, train_acc = 0.800 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.416 (0.361 sec)\n",
      "New Best Accuracy 0.416 > Old Best 0.407.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 19000 \t loss = 0.705, train_acc = 0.600 (0.132 sec/step)\n",
      "step 20000 \t loss = 0.497, train_acc = 0.900 (0.130 sec/step)\n",
      "VALIDATION \t acc = 0.407 (0.348 sec)\n",
      "step 21000 \t loss = 0.147, train_acc = 1.000 (0.132 sec/step)\n",
      "step 22000 \t loss = 0.497, train_acc = 0.900 (0.134 sec/step)\n",
      "VALIDATION \t acc = 0.389 (0.346 sec)\n",
      "step 23000 \t loss = 1.895, train_acc = 0.700 (0.133 sec/step)\n",
      "step 24000 \t loss = 0.745, train_acc = 0.800 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.411 (0.364 sec)\n",
      "step 25000 \t loss = 0.925, train_acc = 0.800 (0.138 sec/step)\n",
      "step 26000 \t loss = 0.041, train_acc = 1.000 (0.131 sec/step)\n",
      "VALIDATION \t acc = 0.404 (0.365 sec)\n",
      "step 27000 \t loss = 1.508, train_acc = 0.700 (0.137 sec/step)\n",
      "step 28000 \t loss = 0.626, train_acc = 0.700 (0.134 sec/step)\n",
      "VALIDATION \t acc = 0.412 (0.339 sec)\n",
      "step 29000 \t loss = 0.703, train_acc = 0.800 (0.135 sec/step)\n",
      "step 30000 \t loss = 0.543, train_acc = 0.700 (0.136 sec/step)\n",
      "VALIDATION \t acc = 0.431 (0.354 sec)\n",
      "New Best Accuracy 0.431 > Old Best 0.416.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 31000 \t loss = 0.692, train_acc = 0.600 (0.131 sec/step)\n",
      "step 32000 \t loss = 1.073, train_acc = 0.800 (0.139 sec/step)\n",
      "VALIDATION \t acc = 0.430 (0.346 sec)\n",
      "step 33000 \t loss = 0.107, train_acc = 1.000 (0.133 sec/step)\n",
      "step 34000 \t loss = 0.071, train_acc = 1.000 (0.138 sec/step)\n",
      "VALIDATION \t acc = 0.411 (0.335 sec)\n",
      "step 35000 \t loss = 0.708, train_acc = 0.900 (0.131 sec/step)\n",
      "step 36000 \t loss = 0.332, train_acc = 0.900 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.406 (0.333 sec)\n",
      "step 37000 \t loss = 0.295, train_acc = 0.900 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.417 (0.354 sec)\n",
      "step 38000 \t loss = 0.743, train_acc = 0.800 (0.143 sec/step)\n",
      "step 39000 \t loss = 0.197, train_acc = 0.900 (0.135 sec/step)\n",
      "VALIDATION \t acc = 0.430 (0.367 sec)\n",
      "step 40000 \t loss = 0.012, train_acc = 1.000 (0.142 sec/step)\n",
      "step 41000 \t loss = 0.267, train_acc = 0.900 (0.135 sec/step)\n",
      "VALIDATION \t acc = 0.398 (0.356 sec)\n",
      "step 42000 \t loss = 0.126, train_acc = 1.000 (0.131 sec/step)\n",
      "step 43000 \t loss = 1.465, train_acc = 0.800 (0.137 sec/step)\n",
      "VALIDATION \t acc = 0.402 (0.348 sec)\n",
      "step 44000 \t loss = 0.112, train_acc = 0.900 (0.144 sec/step)\n",
      "step 45000 \t loss = 0.871, train_acc = 0.900 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.438 (0.356 sec)\n",
      "New Best Accuracy 0.438 > Old Best 0.431.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 46000 \t loss = 0.653, train_acc = 0.800 (0.134 sec/step)\n",
      "step 47000 \t loss = 0.875, train_acc = 0.700 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.410 (0.342 sec)\n",
      "step 48000 \t loss = 0.033, train_acc = 1.000 (0.134 sec/step)\n",
      "step 49000 \t loss = 0.312, train_acc = 0.900 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.441 (0.340 sec)\n",
      "New Best Accuracy 0.441 > Old Best 0.438.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 50000 \t loss = 0.519, train_acc = 0.900 (0.136 sec/step)\n",
      "step 51000 \t loss = 0.944, train_acc = 0.800 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.440 (0.349 sec)\n",
      "step 52000 \t loss = 0.002, train_acc = 1.000 (0.134 sec/step)\n",
      "step 53000 \t loss = 0.089, train_acc = 1.000 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.439 (0.342 sec)\n",
      "step 54000 \t loss = 0.011, train_acc = 1.000 (0.132 sec/step)\n",
      "step 55000 \t loss = 0.234, train_acc = 0.900 (0.136 sec/step)\n",
      "VALIDATION \t acc = 0.417 (0.356 sec)\n",
      "step 56000 \t loss = 0.004, train_acc = 1.000 (0.134 sec/step)\n",
      "VALIDATION \t acc = 0.412 (0.348 sec)\n",
      "step 57000 \t loss = 0.727, train_acc = 0.900 (0.142 sec/step)\n",
      "step 58000 \t loss = 0.310, train_acc = 0.900 (0.130 sec/step)\n",
      "VALIDATION \t acc = 0.418 (0.356 sec)\n",
      "step 59000 \t loss = 0.345, train_acc = 0.800 (0.133 sec/step)\n",
      "step 60000 \t loss = 0.607, train_acc = 0.900 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.426 (0.361 sec)\n",
      "step 61000 \t loss = 0.236, train_acc = 0.800 (0.132 sec/step)\n",
      "step 62000 \t loss = 1.758, train_acc = 0.800 (0.131 sec/step)\n",
      "VALIDATION \t acc = 0.444 (0.341 sec)\n",
      "New Best Accuracy 0.444 > Old Best 0.441.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 63000 \t loss = 0.114, train_acc = 1.000 (0.133 sec/step)\n",
      "step 64000 \t loss = 0.149, train_acc = 0.900 (0.139 sec/step)\n",
      "VALIDATION \t acc = 0.424 (0.349 sec)\n",
      "step 65000 \t loss = 0.198, train_acc = 0.900 (0.136 sec/step)\n",
      "step 66000 \t loss = 0.035, train_acc = 1.000 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.415 (0.340 sec)\n",
      "step 67000 \t loss = 0.052, train_acc = 1.000 (0.138 sec/step)\n",
      "step 68000 \t loss = 0.316, train_acc = 0.900 (0.131 sec/step)\n",
      "VALIDATION \t acc = 0.424 (0.339 sec)\n",
      "step 69000 \t loss = 0.432, train_acc = 0.800 (0.131 sec/step)\n",
      "step 70000 \t loss = 0.091, train_acc = 1.000 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.426 (0.361 sec)\n",
      "step 71000 \t loss = 0.234, train_acc = 0.900 (0.134 sec/step)\n",
      "step 72000 \t loss = 0.001, train_acc = 1.000 (0.134 sec/step)\n",
      "VALIDATION \t acc = 0.416 (0.336 sec)\n",
      "step 73000 \t loss = 0.185, train_acc = 0.900 (0.133 sec/step)\n",
      "step 74000 \t loss = 0.079, train_acc = 1.000 (0.134 sec/step)\n",
      "VALIDATION \t acc = 0.409 (0.352 sec)\n",
      "step 75000 \t loss = 0.169, train_acc = 1.000 (0.137 sec/step)\n",
      "VALIDATION \t acc = 0.423 (0.336 sec)\n",
      "step 76000 \t loss = 0.281, train_acc = 0.900 (0.148 sec/step)\n",
      "step 77000 \t loss = 0.247, train_acc = 0.900 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.425 (0.342 sec)\n",
      "step 78000 \t loss = 0.024, train_acc = 1.000 (0.133 sec/step)\n",
      "step 79000 \t loss = 0.049, train_acc = 1.000 (0.140 sec/step)\n",
      "VALIDATION \t acc = 0.416 (0.351 sec)\n",
      "step 80000 \t loss = 0.447, train_acc = 0.900 (0.134 sec/step)\n",
      "step 81000 \t loss = 0.784, train_acc = 0.900 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.431 (0.348 sec)\n",
      "step 82000 \t loss = 0.427, train_acc = 0.800 (0.137 sec/step)\n",
      "step 83000 \t loss = 0.069, train_acc = 1.000 (0.165 sec/step)\n",
      "VALIDATION \t acc = 0.415 (0.358 sec)\n",
      "step 84000 \t loss = 0.712, train_acc = 0.900 (0.132 sec/step)\n",
      "step 85000 \t loss = 0.202, train_acc = 1.000 (0.135 sec/step)\n",
      "VALIDATION \t acc = 0.389 (0.349 sec)\n",
      "step 86000 \t loss = 1.132, train_acc = 0.900 (0.135 sec/step)\n",
      "step 87000 \t loss = 0.796, train_acc = 0.700 (0.137 sec/step)\n",
      "VALIDATION \t acc = 0.401 (0.349 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 88000 \t loss = 0.071, train_acc = 1.000 (0.132 sec/step)\n",
      "step 89000 \t loss = 1.746, train_acc = 0.400 (0.131 sec/step)\n",
      "VALIDATION \t acc = 0.428 (0.348 sec)\n",
      "step 90000 \t loss = 0.643, train_acc = 0.900 (0.133 sec/step)\n",
      "step 91000 \t loss = 0.005, train_acc = 1.000 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.420 (0.346 sec)\n",
      "step 92000 \t loss = 0.007, train_acc = 1.000 (0.131 sec/step)\n",
      "step 93000 \t loss = 0.206, train_acc = 0.900 (0.130 sec/step)\n",
      "VALIDATION \t acc = 0.400 (0.349 sec)\n",
      "step 94000 \t loss = 0.317, train_acc = 0.800 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.425 (0.359 sec)\n",
      "step 95000 \t loss = 0.823, train_acc = 0.800 (0.148 sec/step)\n",
      "step 96000 \t loss = 0.171, train_acc = 0.900 (0.137 sec/step)\n",
      "VALIDATION \t acc = 0.425 (0.354 sec)\n",
      "step 97000 \t loss = 0.012, train_acc = 1.000 (0.130 sec/step)\n",
      "step 98000 \t loss = 0.825, train_acc = 0.700 (0.131 sec/step)\n",
      "VALIDATION \t acc = 0.421 (0.357 sec)\n",
      "step 99000 \t loss = 0.087, train_acc = 1.000 (0.134 sec/step)\n",
      "step 100000 \t loss = 0.008, train_acc = 1.000 (0.131 sec/step)\n",
      "VALIDATION \t acc = 0.400 (0.354 sec)\n",
      "step 101000 \t loss = 0.544, train_acc = 0.900 (0.132 sec/step)\n",
      "step 102000 \t loss = 0.013, train_acc = 1.000 (0.136 sec/step)\n",
      "VALIDATION \t acc = 0.383 (0.356 sec)\n",
      "step 103000 \t loss = 0.595, train_acc = 0.800 (0.131 sec/step)\n",
      "step 104000 \t loss = 0.474, train_acc = 0.800 (0.144 sec/step)\n",
      "VALIDATION \t acc = 0.415 (0.358 sec)\n",
      "step 105000 \t loss = 0.649, train_acc = 0.900 (0.136 sec/step)\n",
      "step 106000 \t loss = 0.216, train_acc = 0.900 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.416 (0.346 sec)\n",
      "step 107000 \t loss = 0.447, train_acc = 0.900 (0.132 sec/step)\n",
      "step 108000 \t loss = 1.377, train_acc = 0.700 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.392 (0.357 sec)\n",
      "step 109000 \t loss = 0.013, train_acc = 1.000 (0.134 sec/step)\n",
      "step 110000 \t loss = 0.301, train_acc = 0.900 (0.133 sec/step)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model Setup\n",
    "'''\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(BATCH_SIZE, 32, 32, 3))\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(BATCH_SIZE))\n",
    "is_training = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "net = feature_extractor_cnn(x, y, is_training)\n",
    "\n",
    "'''\n",
    "Loss, Metrics, and Optimization Setup\n",
    "'''\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, #GT probability distribution\n",
    "        logits=net[-1], # unscaled log prob\n",
    "        name='sparse_softmax_cross_entropy')\n",
    "\n",
    "reduced_loss = tf.reduce_mean(loss)\n",
    "train_loss_summary = tf.summary.scalar('training_loss', reduced_loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        beta1=BETA1,\n",
    "        beta2=BETA2,\n",
    "        name='AdamOptimizer')\n",
    "train_op = optimizer.minimize(reduced_loss)\n",
    "\n",
    "pred = tf.nn.softmax(\n",
    "        logits=net[-1],\n",
    "        name='softmax')\n",
    "pred_class = tf.cast(tf.argmax(pred, axis=1), tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(\n",
    "            tf.equal(y, pred_class),\n",
    "        tf.float32))\n",
    "\n",
    "train_acc_summary = tf.summary.scalar('training_accuracy', acc)\n",
    "\n",
    "\n",
    "'''\n",
    "TensorBoard Setup\n",
    "'''\n",
    "all_train_summary = tf.summary.merge_all()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(SNAPSHOT_DIR,\n",
    "        graph=tf.get_default_graph())\n",
    "\n",
    "'''\n",
    "Tensorflow Saver Setup\n",
    "'''\n",
    "saver = tf.train.Saver(var_list=tf.global_variables(),\n",
    "                       max_to_keep=SNAPSHOT_MAX)\n",
    "\n",
    "'''\n",
    "Tensorflow Session Setup\n",
    "'''\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "init = tf.group(tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer())\n",
    "sess.run(init)\n",
    "\n",
    "'''\n",
    "Declare Validation Loop\n",
    "'''\n",
    "def run_validation():\n",
    "    global best_acc\n",
    "    \n",
    "    start_t = time()\n",
    "    overall_acc = 0\n",
    "    overall_loss = 0\n",
    "    for j in range(0, n_validation, BATCH_SIZE):\n",
    "        # Assemble Batch\n",
    "        idx = validation_indices[j:(j+BATCH_SIZE)]\n",
    "        x_batch = data_x[idx,...]\n",
    "        y_batch = data_y[idx,...]\n",
    "        \n",
    "        feed_dict = {x:x_batch,\n",
    "                 y:y_batch,\n",
    "                 is_training: False}\n",
    "        loss_v, acc_v, pred_v = sess.run(\n",
    "                [reduced_loss, acc, pred],\n",
    "                feed_dict=feed_dict)\n",
    "        overall_acc += acc_v\n",
    "        overall_loss += loss_v\n",
    "        \n",
    "        \n",
    "    duration = time() - start_t\n",
    "    overall_acc /= (n_validation / BATCH_SIZE)\n",
    "    overall_loss /= (n_validation / BATCH_SIZE)\n",
    "    \n",
    "    overall_acc_summary = tf.Summary()\n",
    "    overall_acc_summary.value.add(tag='validation_accuracy', simple_value=overall_acc)\n",
    "    overall_loss_summary = tf.Summary()\n",
    "    overall_loss_summary.value.add(tag='validation_loss', simple_value=overall_loss)\n",
    "\n",
    "    summary_writer.add_summary(overall_acc_summary, step)\n",
    "    summary_writer.add_summary(overall_loss_summary, step)\n",
    "    \n",
    "    print('VALIDATION \\t acc = {:.3f} ({:.3f} sec)'.format(\n",
    "                overall_acc, duration))\n",
    "    if overall_acc > best_acc:\n",
    "        print('New Best Accuracy {:.3f} > Old Best {:.3f}.  Saving...'.format(\n",
    "                overall_acc, best_acc))\n",
    "        best_acc = overall_acc\n",
    "        save(saver, sess, SNAPSHOT_DIR, step)\n",
    "        \n",
    "'''\n",
    "Main Training Loop\n",
    "'''\n",
    "step = 0\n",
    "epoch = 0\n",
    "best_acc = 0\n",
    "while epoch < NUM_EPOCH:\n",
    "    step += 1\n",
    "    # Allocate Space For Batch\n",
    "    x_batch = np.zeros((BATCH_SIZE,) + data_x.shape[1:], dtype=np.float32)\n",
    "    y_batch = np.zeros((BATCH_SIZE,) + data_y.shape[1:], dtype=np.int32)\n",
    "    \n",
    "    # Run Validation\n",
    "    if step % batches_per_epoch == 0:\n",
    "        epoch += 1\n",
    "        run_validation()\n",
    "        \n",
    "    # Form Training Batch\n",
    "    start_t = time()\n",
    "    for i in range(BATCH_SIZE):\n",
    "        idx = next(train_indices)\n",
    "        x_batch[i,...] = data_x[idx, ...]\n",
    "        y_batch[i,...] = data_y[idx, ...]\n",
    "        \n",
    "    # Prepare Feed Dictionary\n",
    "    feed_dict = {x:x_batch,\n",
    "                 y:y_batch,\n",
    "                 is_training: True}\n",
    "    # Run Training Summary\n",
    "    if step % SUMMARY_EVERY == 0:\n",
    "        loss_v, _, summary_v, acc_v, pred_v = sess.run(\n",
    "                [reduced_loss, train_op, all_train_summary, acc, pred],\n",
    "                feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_v, step)\n",
    "        duration = time() - start_t\n",
    "        print('step {:d} \\t loss = {:.3f}, train_acc = {:.3f} ({:.3f} sec/step)'.format(\n",
    "                step, loss_v, acc_v, duration))\n",
    "    else: # Run Simple Train\n",
    "        loss_v, _ = sess.run([reduced_loss, train_op],\n",
    "                feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
