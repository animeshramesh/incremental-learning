{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo:\n",
    "- Experiment with normalization while creating triplets\n",
    "- Do you need dropout in the regressor network?\n",
    "- Simultaneous feature learning?\n",
    "- See when are results bad/equal/better? See which classes they correspond to.. \n",
    "\n",
    "# Todo on TF model\n",
    "- implement data import\n",
    "- implement data normalization\n",
    "- debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from utils import optimistic_restore, save\n",
    "import layers\n",
    "\n",
    "PWD = os.getcwd()\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(PWD, '..')))\n",
    "import pickle_utils\n",
    "import cifar_utils\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "HYPERPARAMS\n",
    "'''\n",
    "BATCH_SIZE = 10\n",
    "DATA_PATH = '/media/red/capstone/data/cifar-100/cifar-custom'\n",
    "LEARNING_RATE = 2.5e-4\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.99\n",
    "NUM_CLASSES = 40\n",
    "NUM_STEPS = 20001\n",
    "RANDOM_SEED = 1234\n",
    "SAVE_PRED_EVERY = 1000\n",
    "SNAPSHOT_MAX = 10\n",
    "SNAPSHOT_DIR = './snapshots/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load custom CIFAR data. \n",
    "'''\n",
    "# cifar_raw = pickle_utils.load(DATA_PATH)\n",
    "custom_dataset = pickle_utils.load(DATA_PATH)\n",
    "\n",
    "data_x, data_y = [], []\n",
    "for label in custom_dataset['training'].keys():\n",
    "    for item in custom_dataset['training'][label]:\n",
    "        data_x.append(item) # 28 x 28 x 3\n",
    "        data_y.append(label) # 0-39\n",
    "data_x = np.stack(data_x)\n",
    "data_y = np.stack(data_y).astype(np.int32)\n",
    "\n",
    "NUM_DATA_POINTS = data_x.shape[0]\n",
    "# Shuffle data\n",
    "random_indices = np.random.permutation(NUM_DATA_POINTS)\n",
    "data_indices = cycle(random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Declare model\n",
    "'''\n",
    "def feature_extractor_cnn(x, y, is_training):\n",
    "    net = []\n",
    "    net.append(layers.conv2d(\n",
    "            input=x,\n",
    "            shape=(3,3,3,32),\n",
    "            padding='SAME',\n",
    "            activation='relu',\n",
    "            name='conv_1_1'\n",
    "            )[0])\n",
    "    net.append(layers.batch_norm(\n",
    "            x=net[-1],\n",
    "            phase=is_training,\n",
    "            name='bn_1_2'))\n",
    "    net.append(layers.conv2d(\n",
    "            input=net[-1],\n",
    "            shape=(3,3,32,32),\n",
    "            padding='SAME',\n",
    "            activation='relu',\n",
    "            bias=False,\n",
    "            name='conv_1_2'\n",
    "            )[0])\n",
    "    net.append(tf.nn.max_pool(\n",
    "            value=net[-1],\n",
    "            ksize=[1, 2, 2, 1],\n",
    "            strides=[1,2,2,1],\n",
    "            padding='SAME',\n",
    "            name='pool_1'\n",
    "            ))\n",
    "    net.append(layers.batch_norm(\n",
    "            x=net[-1],\n",
    "            phase=is_training,\n",
    "            name='bn_2_1'))\n",
    "    net.append(layers.conv2d(\n",
    "            input=net[-1],\n",
    "            shape=(3,3,32,64),\n",
    "            padding='SAME',\n",
    "            activation='relu',\n",
    "            bias=False,\n",
    "            name='conv_2_1'\n",
    "            )[0])\n",
    "    net.append(layers.batch_norm(\n",
    "            x=net[-1],\n",
    "            phase=is_training,\n",
    "            name='bn_2_2'))\n",
    "    net.append(layers.conv2d(\n",
    "            input=net[-1],\n",
    "            shape=(3,3,64,64),\n",
    "            padding='SAME',\n",
    "            activation='relu',\n",
    "            bias=False,\n",
    "            name='conv_2_2'\n",
    "            )[0])\n",
    "    net.append(tf.nn.max_pool(\n",
    "            value=net[-1],\n",
    "            ksize=[1, 2, 2, 1],\n",
    "            strides=[1,2,2,1],\n",
    "            padding='SAME',\n",
    "            name='pool_2'\n",
    "            ))\n",
    "\n",
    "    net.append(tf.contrib.layers.flatten(\n",
    "            inputs=net[-1],\n",
    "            scope='flat_1'\n",
    "            ))\n",
    "    net.append(layers.fc(\n",
    "            input=net[-1],\n",
    "            units=1024,\n",
    "            activation='relu',\n",
    "            name='fc_1'\n",
    "            )[0])\n",
    "    net.append(tf.nn.dropout(\n",
    "            x=net[-1],\n",
    "            keep_prob=0.5,\n",
    "            name='dropout_fc_1'\n",
    "            ))\n",
    "    net.append(layers.fc(\n",
    "            input=net[-1],\n",
    "            units=1024,\n",
    "            activation='relu',\n",
    "            name='fc_2'\n",
    "            )[0])\n",
    "    net.append(tf.nn.dropout(\n",
    "            x=net[-1],\n",
    "            keep_prob=0.5,\n",
    "            name='dropout_fc_2'\n",
    "            ))\n",
    "    net.append(layers.fc(\n",
    "            input=net[-1],\n",
    "            units=NUM_CLASSES,\n",
    "            activation='linear',\n",
    "            name='fc_3'\n",
    "            )[0])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The checkpoint has been created.\n",
      "step 0 \t loss = 5.447, (0.572 sec/step)\n",
      "The checkpoint has been created.\n",
      "step 1000 \t loss = 3.659, (0.255 sec/step)\n",
      "The checkpoint has been created.\n",
      "step 2000 \t loss = 3.537, (0.303 sec/step)\n",
      "The checkpoint has been created.\n",
      "step 3000 \t loss = 3.299, (0.259 sec/step)\n",
      "The checkpoint has been created.\n",
      "step 4000 \t loss = 2.646, (0.251 sec/step)\n",
      "The checkpoint has been created.\n",
      "step 5000 \t loss = 2.476, (0.246 sec/step)\n",
      "The checkpoint has been created.\n",
      "step 6000 \t loss = 2.195, (0.246 sec/step)\n",
      "The checkpoint has been created.\n",
      "step 7000 \t loss = 1.825, (0.246 sec/step)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6dea6d965b84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         loss_v, _ = sess.run([reduced_loss, train_op],\n\u001b[0;32m---> 87\u001b[0;31m                 feed_dict=feed_dict)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model Setup\n",
    "'''\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(BATCH_SIZE, 32, 32, 3))\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(BATCH_SIZE))\n",
    "is_training = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "net = feature_extractor_cnn(x, y, is_training)\n",
    "\n",
    "pred = tf.nn.softmax(\n",
    "        logits=net[-1],\n",
    "        name='softmax')\n",
    "\n",
    "'''\n",
    "Loss and Optimization Setup\n",
    "'''\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, #GT probability distribution\n",
    "        logits=net[-1], # unscaled log prob\n",
    "        name='sparse_softmax_cross_entropy')\n",
    "\n",
    "reduced_loss = tf.reduce_mean(loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        beta1=BETA1,\n",
    "        beta2=BETA2,\n",
    "        name='AdamOptimizer')\n",
    "train_op = optimizer.minimize(reduced_loss)\n",
    "\n",
    "'''\n",
    "TensorBoard Setup\n",
    "'''\n",
    "all_summary = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(SNAPSHOT_DIR,\n",
    "        graph=tf.get_default_graph())\n",
    "\n",
    "'''\n",
    "Tensorflow Saver Setup\n",
    "'''\n",
    "saver = tf.train.Saver(var_list=tf.global_variables(),\n",
    "                       max_to_keep=SNAPSHOT_MAX)\n",
    "\n",
    "'''\n",
    "Tensorflow Session Setup\n",
    "'''\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "init = tf.group(tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer())\n",
    "sess.run(init)\n",
    "\n",
    "'''\n",
    "Main Training Loop\n",
    "'''\n",
    "for step in range(NUM_STEPS):\n",
    "    start_t = time()\n",
    "    \n",
    "    # Create Batch\n",
    "    x_batch = np.zeros((BATCH_SIZE,) + data_x.shape[1:])\n",
    "    y_batch = np.zeros((BATCH_SIZE,) + data_y.shape[1:])\n",
    "    \n",
    "    for i in range(BATCH_SIZE):\n",
    "        idx = next(data_indices)\n",
    "        x_batch[i,...] = data_x[idx, ...]\n",
    "        y_batch[i,...] = data_y[idx, ...]\n",
    "        \n",
    "    feed_dict = {x:x_batch,\n",
    "                 y:y_batch,\n",
    "                 is_training: True}\n",
    "    \n",
    "    # Run Graph\n",
    "    if step % SAVE_PRED_EVERY == 0:\n",
    "        loss_v, _, summary_v  = sess.run([reduced_loss, train_op, all_summary],\n",
    "                feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_v, step)\n",
    "        save(saver, sess, SNAPSHOT_DIR, step)\n",
    "        duration = time() - start_t\n",
    "        print('step {:d} \\t loss = {:.3f}, ({:.3f} sec/step)'.format(\n",
    "        step, loss_v, duration))\n",
    "    else:\n",
    "        loss_v, _ = sess.run([reduced_loss, train_op],\n",
    "                feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
