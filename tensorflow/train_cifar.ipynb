{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo:\n",
    "- Experiment with normalization while creating triplets\n",
    "- Do you need dropout in the regressor network?\n",
    "- Simultaneous feature learning?\n",
    "- See when are results bad/equal/better? See which classes they correspond to.. \n",
    "\n",
    "# Todo on TF model\n",
    "- implement data import\n",
    "- implement data normalization\n",
    "- debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import cycle\n",
    "import random\n",
    "\n",
    "from utils import optimistic_restore, save\n",
    "import layers\n",
    "\n",
    "PWD = os.getcwd()\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(PWD, '..')))\n",
    "import pickle_utils\n",
    "import cifar_utils\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "HYPERPARAMS\n",
    "'''\n",
    "BATCH_SIZE = 10\n",
    "DATA_PATH = '/media/red/capstone/data/cifar-100/cifar-custom'\n",
    "LEARNING_RATE = 1e-4\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.99\n",
    "NUM_CLASSES = 40\n",
    "NUM_EPOCH = 100\n",
    "RANDOM_SEED = 1234\n",
    "SUMMARY_EVERY = 1000\n",
    "VALIDATION_PERCENTAGE = 0.05\n",
    "SNAPSHOT_MAX = 10 # Keeps the last best 10 snapshots (best determined by validation accuracy)\n",
    "SNAPSHOT_DIR = '/media/red/capstone/snapshots/feature_extractor_cnn_lr_augment'\n",
    "\n",
    "np.random.seed(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load custom CIFAR data. \n",
    "'''\n",
    "# cifar_raw = pickle_utils.load(DATA_PATH)\n",
    "custom_dataset = pickle_utils.load(DATA_PATH)\n",
    "\n",
    "data_x, data_y = [], []\n",
    "for label in custom_dataset['training'].keys():\n",
    "    for item in custom_dataset['training'][label]:\n",
    "        data_x.append(item) # 28 x 28 x 3\n",
    "        data_y.append(label) # 0-39\n",
    "data_x = np.stack(data_x).astype(np.float32)\n",
    "data_y = np.stack(data_y).astype(np.int32)\n",
    "\n",
    "# Normalize x\n",
    "data_x = (data_x / 255.0) - 0.5\n",
    "\n",
    "def round_to(n, precision):\n",
    "    return int( n/precision+0.5 ) * precision\n",
    "\n",
    "n_total_data = data_x.shape[0]\n",
    "n_validation = round_to(VALIDATION_PERCENTAGE * n_total_data, BATCH_SIZE)\n",
    "batches_per_epoch = np.round((n_total_data - n_validation) / BATCH_SIZE)\n",
    "# Shuffle data\n",
    "random_indices = np.random.permutation(n_total_data)\n",
    "train_indices = cycle(random_indices[n_validation:])\n",
    "validation_indices = random_indices[:n_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Declare model\n",
    "'''\n",
    "def feature_extractor_cnn(x, y, is_training):\n",
    "    net = []\n",
    "    net.append(layers.conv2d(\n",
    "            input=x,\n",
    "            shape=(3,3,3,32),\n",
    "            padding='SAME',\n",
    "            activation='relu',\n",
    "            name='conv_1_1'\n",
    "            )[0])\n",
    "    net.append(layers.batch_norm(\n",
    "            x=net[-1],\n",
    "            phase=is_training,\n",
    "            name='bn_1_2'))\n",
    "    net.append(layers.conv2d(\n",
    "            input=net[-1],\n",
    "            shape=(3,3,32,32),\n",
    "            padding='SAME',\n",
    "            activation='relu',\n",
    "            bias=False,\n",
    "            name='conv_1_2'\n",
    "            )[0])\n",
    "    net.append(tf.nn.max_pool(\n",
    "            value=net[-1],\n",
    "            ksize=[1, 2, 2, 1],\n",
    "            strides=[1,2,2,1],\n",
    "            padding='SAME',\n",
    "            name='pool_1'\n",
    "            ))\n",
    "    net.append(layers.batch_norm(\n",
    "            x=net[-1],\n",
    "            phase=is_training,\n",
    "            name='bn_2_1'))\n",
    "    net.append(layers.conv2d(\n",
    "            input=net[-1],\n",
    "            shape=(3,3,32,64),\n",
    "            padding='SAME',\n",
    "            activation='relu',\n",
    "            bias=False,\n",
    "            name='conv_2_1'\n",
    "            )[0])\n",
    "    net.append(layers.batch_norm(\n",
    "            x=net[-1],\n",
    "            phase=is_training,\n",
    "            name='bn_2_2'))\n",
    "    net.append(layers.conv2d(\n",
    "            input=net[-1],\n",
    "            shape=(3,3,64,64),\n",
    "            padding='SAME',\n",
    "            activation='relu',\n",
    "            bias=False,\n",
    "            name='conv_2_2'\n",
    "            )[0])\n",
    "    net.append(tf.nn.max_pool(\n",
    "            value=net[-1],\n",
    "            ksize=[1, 2, 2, 1],\n",
    "            strides=[1,2,2,1],\n",
    "            padding='SAME',\n",
    "            name='pool_2'\n",
    "            ))\n",
    "\n",
    "    net.append(tf.contrib.layers.flatten(\n",
    "            inputs=net[-1],\n",
    "            scope='flat_1'\n",
    "            ))\n",
    "    net.append(layers.fc(\n",
    "            input=net[-1],\n",
    "            units=1024,\n",
    "            activation='relu',\n",
    "            name='fc_1'\n",
    "            )[0])\n",
    "    net.append(tf.nn.dropout(\n",
    "            x=net[-1],\n",
    "            keep_prob=0.5,\n",
    "            name='dropout_fc_1'\n",
    "            ))\n",
    "    net.append(layers.fc(\n",
    "            input=net[-1],\n",
    "            units=1024,\n",
    "            activation='relu',\n",
    "            name='fc_2'\n",
    "            )[0])\n",
    "    net.append(tf.nn.dropout(\n",
    "            x=net[-1],\n",
    "            keep_prob=0.5,\n",
    "            name='dropout_fc_2'\n",
    "            ))\n",
    "    net.append(layers.fc(\n",
    "            input=net[-1],\n",
    "            units=NUM_CLASSES,\n",
    "            activation='linear',\n",
    "            name='fc_3'\n",
    "            )[0])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000 \t loss = 2.958, train_acc = 0.100 (0.165 sec/step)\n",
      "VALIDATION \t acc = 0.108 (0.359 sec)\n",
      "New Best Accuracy 0.108 > Old Best 0.000.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 2000 \t loss = 3.456, train_acc = 0.100 (0.129 sec/step)\n",
      "step 3000 \t loss = 3.319, train_acc = 0.200 (0.126 sec/step)\n",
      "VALIDATION \t acc = 0.164 (0.348 sec)\n",
      "New Best Accuracy 0.164 > Old Best 0.108.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 4000 \t loss = 3.048, train_acc = 0.000 (0.126 sec/step)\n",
      "step 5000 \t loss = 2.610, train_acc = 0.200 (0.128 sec/step)\n",
      "VALIDATION \t acc = 0.195 (0.350 sec)\n",
      "New Best Accuracy 0.195 > Old Best 0.164.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 6000 \t loss = 2.943, train_acc = 0.200 (0.130 sec/step)\n",
      "step 7000 \t loss = 1.999, train_acc = 0.500 (0.128 sec/step)\n",
      "VALIDATION \t acc = 0.229 (0.349 sec)\n",
      "New Best Accuracy 0.229 > Old Best 0.195.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 8000 \t loss = 3.693, train_acc = 0.000 (0.127 sec/step)\n",
      "step 9000 \t loss = 2.546, train_acc = 0.400 (0.128 sec/step)\n",
      "VALIDATION \t acc = 0.257 (0.342 sec)\n",
      "New Best Accuracy 0.257 > Old Best 0.229.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 10000 \t loss = 2.607, train_acc = 0.200 (0.130 sec/step)\n",
      "step 11000 \t loss = 2.714, train_acc = 0.100 (0.128 sec/step)\n",
      "VALIDATION \t acc = 0.281 (0.362 sec)\n",
      "New Best Accuracy 0.281 > Old Best 0.257.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 12000 \t loss = 3.200, train_acc = 0.200 (0.127 sec/step)\n",
      "step 13000 \t loss = 3.410, train_acc = 0.100 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.296 (0.343 sec)\n",
      "New Best Accuracy 0.296 > Old Best 0.281.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 14000 \t loss = 3.181, train_acc = 0.200 (0.128 sec/step)\n",
      "step 15000 \t loss = 1.599, train_acc = 0.500 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.319 (0.343 sec)\n",
      "New Best Accuracy 0.319 > Old Best 0.296.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 16000 \t loss = 1.635, train_acc = 0.400 (0.134 sec/step)\n",
      "step 17000 \t loss = 2.364, train_acc = 0.300 (0.128 sec/step)\n",
      "VALIDATION \t acc = 0.330 (0.355 sec)\n",
      "New Best Accuracy 0.330 > Old Best 0.319.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 18000 \t loss = 2.717, train_acc = 0.200 (0.130 sec/step)\n",
      "VALIDATION \t acc = 0.325 (0.368 sec)\n",
      "step 19000 \t loss = 3.187, train_acc = 0.000 (0.139 sec/step)\n",
      "step 20000 \t loss = 1.196, train_acc = 0.600 (0.128 sec/step)\n",
      "VALIDATION \t acc = 0.334 (0.381 sec)\n",
      "New Best Accuracy 0.334 > Old Best 0.330.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 21000 \t loss = 1.491, train_acc = 0.600 (0.129 sec/step)\n",
      "step 22000 \t loss = 2.122, train_acc = 0.300 (0.130 sec/step)\n",
      "VALIDATION \t acc = 0.354 (0.361 sec)\n",
      "New Best Accuracy 0.354 > Old Best 0.334.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 23000 \t loss = 1.820, train_acc = 0.500 (0.129 sec/step)\n",
      "step 24000 \t loss = 2.207, train_acc = 0.400 (0.129 sec/step)\n",
      "VALIDATION \t acc = 0.320 (0.338 sec)\n",
      "step 25000 \t loss = 2.815, train_acc = 0.200 (0.132 sec/step)\n",
      "step 26000 \t loss = 0.924, train_acc = 0.600 (0.130 sec/step)\n",
      "VALIDATION \t acc = 0.355 (0.339 sec)\n",
      "New Best Accuracy 0.355 > Old Best 0.354.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 27000 \t loss = 2.443, train_acc = 0.200 (0.127 sec/step)\n",
      "step 28000 \t loss = 1.945, train_acc = 0.400 (0.128 sec/step)\n",
      "VALIDATION \t acc = 0.352 (0.358 sec)\n",
      "step 29000 \t loss = 2.004, train_acc = 0.400 (0.128 sec/step)\n",
      "step 30000 \t loss = 1.635, train_acc = 0.500 (0.138 sec/step)\n",
      "VALIDATION \t acc = 0.385 (0.343 sec)\n",
      "New Best Accuracy 0.385 > Old Best 0.355.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 31000 \t loss = 2.243, train_acc = 0.200 (0.129 sec/step)\n",
      "step 32000 \t loss = 2.603, train_acc = 0.300 (0.131 sec/step)\n",
      "VALIDATION \t acc = 0.389 (0.372 sec)\n",
      "New Best Accuracy 0.389 > Old Best 0.385.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 33000 \t loss = 2.131, train_acc = 0.400 (0.138 sec/step)\n",
      "step 34000 \t loss = 0.363, train_acc = 1.000 (0.130 sec/step)\n",
      "VALIDATION \t acc = 0.374 (0.352 sec)\n",
      "step 35000 \t loss = 1.552, train_acc = 0.400 (0.128 sec/step)\n",
      "step 36000 \t loss = 1.569, train_acc = 0.400 (0.127 sec/step)\n",
      "VALIDATION \t acc = 0.369 (0.366 sec)\n",
      "step 37000 \t loss = 1.342, train_acc = 0.500 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.374 (0.371 sec)\n",
      "step 38000 \t loss = 1.398, train_acc = 0.600 (0.139 sec/step)\n",
      "step 39000 \t loss = 1.125, train_acc = 0.800 (0.134 sec/step)\n",
      "VALIDATION \t acc = 0.358 (0.354 sec)\n",
      "step 40000 \t loss = 0.605, train_acc = 0.700 (0.128 sec/step)\n",
      "step 41000 \t loss = 1.459, train_acc = 0.300 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.390 (0.369 sec)\n",
      "New Best Accuracy 0.390 > Old Best 0.389.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 42000 \t loss = 1.730, train_acc = 0.600 (0.131 sec/step)\n",
      "step 43000 \t loss = 1.098, train_acc = 0.500 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.365 (0.351 sec)\n",
      "step 44000 \t loss = 2.162, train_acc = 0.300 (0.128 sec/step)\n",
      "step 45000 \t loss = 0.256, train_acc = 0.900 (0.129 sec/step)\n",
      "VALIDATION \t acc = 0.382 (0.368 sec)\n",
      "step 46000 \t loss = 1.893, train_acc = 0.500 (0.130 sec/step)\n",
      "step 47000 \t loss = 1.160, train_acc = 0.700 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.386 (0.346 sec)\n",
      "step 48000 \t loss = 0.561, train_acc = 0.800 (0.131 sec/step)\n",
      "step 49000 \t loss = 1.678, train_acc = 0.500 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.397 (0.353 sec)\n",
      "New Best Accuracy 0.397 > Old Best 0.390.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 50000 \t loss = 1.100, train_acc = 0.700 (0.137 sec/step)\n",
      "step 51000 \t loss = 1.221, train_acc = 0.500 (0.128 sec/step)\n",
      "VALIDATION \t acc = 0.373 (0.342 sec)\n",
      "step 52000 \t loss = 2.000, train_acc = 0.500 (0.129 sec/step)\n",
      "step 53000 \t loss = 0.058, train_acc = 1.000 (0.145 sec/step)\n",
      "VALIDATION \t acc = 0.365 (0.353 sec)\n",
      "step 54000 \t loss = 0.734, train_acc = 0.700 (0.139 sec/step)\n",
      "step 55000 \t loss = 1.897, train_acc = 0.600 (0.135 sec/step)\n",
      "VALIDATION \t acc = 0.376 (0.360 sec)\n",
      "step 56000 \t loss = 0.403, train_acc = 0.800 (0.135 sec/step)\n",
      "VALIDATION \t acc = 0.400 (0.356 sec)\n",
      "New Best Accuracy 0.400 > Old Best 0.397.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 57000 \t loss = 0.403, train_acc = 1.000 (0.130 sec/step)\n",
      "step 58000 \t loss = 0.733, train_acc = 0.700 (0.143 sec/step)\n",
      "VALIDATION \t acc = 0.377 (0.346 sec)\n",
      "step 59000 \t loss = 0.520, train_acc = 0.800 (0.132 sec/step)\n",
      "step 60000 \t loss = 0.287, train_acc = 1.000 (0.129 sec/step)\n",
      "VALIDATION \t acc = 0.381 (0.350 sec)\n",
      "step 61000 \t loss = 0.543, train_acc = 0.800 (0.133 sec/step)\n",
      "step 62000 \t loss = 0.908, train_acc = 0.700 (0.129 sec/step)\n",
      "VALIDATION \t acc = 0.394 (0.341 sec)\n",
      "step 63000 \t loss = 0.377, train_acc = 0.900 (0.132 sec/step)\n",
      "step 64000 \t loss = 0.036, train_acc = 1.000 (0.137 sec/step)\n",
      "VALIDATION \t acc = 0.395 (0.342 sec)\n",
      "step 65000 \t loss = 1.122, train_acc = 0.500 (0.130 sec/step)\n",
      "step 66000 \t loss = 0.739, train_acc = 0.700 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.379 (0.343 sec)\n",
      "step 67000 \t loss = 0.884, train_acc = 0.800 (0.132 sec/step)\n",
      "step 68000 \t loss = 0.411, train_acc = 0.900 (0.133 sec/step)\n",
      "VALIDATION \t acc = 0.390 (0.340 sec)\n",
      "step 69000 \t loss = 1.027, train_acc = 0.600 (0.133 sec/step)\n",
      "step 70000 \t loss = 0.919, train_acc = 0.800 (0.132 sec/step)\n",
      "VALIDATION \t acc = 0.380 (0.339 sec)\n",
      "step 71000 \t loss = 0.781, train_acc = 0.800 (0.132 sec/step)\n",
      "step 72000 \t loss = 0.399, train_acc = 0.900 (0.129 sec/step)\n",
      "VALIDATION \t acc = 0.415 (0.334 sec)\n",
      "New Best Accuracy 0.415 > Old Best 0.400.  Saving...\n",
      "The checkpoint has been created.\n",
      "step 73000 \t loss = 0.333, train_acc = 0.800 (0.130 sec/step)\n",
      "step 74000 \t loss = 1.126, train_acc = 0.500 (0.135 sec/step)\n",
      "VALIDATION \t acc = 0.375 (0.345 sec)\n",
      "step 75000 \t loss = 0.410, train_acc = 0.900 (0.134 sec/step)\n",
      "VALIDATION \t acc = 0.369 (0.343 sec)\n",
      "step 76000 \t loss = 0.387, train_acc = 0.800 (0.141 sec/step)\n",
      "step 77000 \t loss = 0.031, train_acc = 1.000 (0.130 sec/step)\n",
      "VALIDATION \t acc = 0.389 (0.363 sec)\n",
      "step 78000 \t loss = 0.147, train_acc = 0.900 (0.135 sec/step)\n",
      "step 79000 \t loss = 0.175, train_acc = 1.000 (0.131 sec/step)\n",
      "VALIDATION \t acc = 0.398 (0.337 sec)\n",
      "step 80000 \t loss = 0.393, train_acc = 0.900 (0.133 sec/step)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b14b3f82d897>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Run Simple Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         loss_v, _ = sess.run([reduced_loss, train_op],\n\u001b[0;32m--> 154\u001b[0;31m                 feed_dict=feed_dict)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model Setup\n",
    "'''\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(BATCH_SIZE, 32, 32, 3))\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(BATCH_SIZE))\n",
    "is_training = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "net = feature_extractor_cnn(x, y, is_training)\n",
    "\n",
    "'''\n",
    "Loss, Metrics, and Optimization Setup\n",
    "'''\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, #GT probability distribution\n",
    "        logits=net[-1], # unscaled log prob\n",
    "        name='sparse_softmax_cross_entropy')\n",
    "\n",
    "reduced_loss = tf.reduce_mean(loss)\n",
    "train_loss_summary = tf.summary.scalar('training_loss', reduced_loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        beta1=BETA1,\n",
    "        beta2=BETA2,\n",
    "        name='AdamOptimizer')\n",
    "train_op = optimizer.minimize(reduced_loss)\n",
    "\n",
    "pred = tf.nn.softmax(\n",
    "        logits=net[-1],\n",
    "        name='softmax')\n",
    "pred_class = tf.cast(tf.argmax(pred, axis=1), tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(\n",
    "            tf.equal(y, pred_class),\n",
    "        tf.float32))\n",
    "\n",
    "train_acc_summary = tf.summary.scalar('training_accuracy', acc)\n",
    "\n",
    "\n",
    "'''\n",
    "TensorBoard Setup\n",
    "'''\n",
    "all_train_summary = tf.summary.merge_all()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(SNAPSHOT_DIR,\n",
    "        graph=tf.get_default_graph())\n",
    "\n",
    "'''\n",
    "Tensorflow Saver Setup\n",
    "'''\n",
    "saver = tf.train.Saver(var_list=tf.global_variables(),\n",
    "                       max_to_keep=SNAPSHOT_MAX)\n",
    "\n",
    "'''\n",
    "Tensorflow Session Setup\n",
    "'''\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "init = tf.group(tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer())\n",
    "sess.run(init)\n",
    "\n",
    "'''\n",
    "Declare Validation Loop\n",
    "'''\n",
    "def run_validation():\n",
    "    global best_acc\n",
    "    \n",
    "    start_t = time()\n",
    "    overall_acc = 0\n",
    "    overall_loss = 0\n",
    "    for j in range(0, n_validation, BATCH_SIZE):\n",
    "        # Assemble Batch\n",
    "        idx = validation_indices[j:(j+BATCH_SIZE)]\n",
    "        x_batch = data_x[idx,...]\n",
    "        y_batch = data_y[idx,...]\n",
    "        \n",
    "        feed_dict = {x:x_batch,\n",
    "                 y:y_batch,\n",
    "                 is_training: False}\n",
    "        loss_v, acc_v, pred_v = sess.run(\n",
    "                [reduced_loss, acc, pred],\n",
    "                feed_dict=feed_dict)\n",
    "        overall_acc += acc_v\n",
    "        overall_loss += loss_v\n",
    "        \n",
    "        \n",
    "    duration = time() - start_t\n",
    "    overall_acc /= (n_validation / BATCH_SIZE)\n",
    "    overall_loss /= (n_validation / BATCH_SIZE)\n",
    "    \n",
    "    overall_acc_summary = tf.Summary()\n",
    "    overall_acc_summary.value.add(tag='validation_accuracy', simple_value=overall_acc)\n",
    "    overall_loss_summary = tf.Summary()\n",
    "    overall_loss_summary.value.add(tag='validation_loss', simple_value=overall_loss)\n",
    "\n",
    "    summary_writer.add_summary(overall_acc_summary, step)\n",
    "    summary_writer.add_summary(overall_loss_summary, step)\n",
    "    \n",
    "    print('VALIDATION \\t acc = {:.3f} ({:.3f} sec)'.format(\n",
    "                overall_acc, duration))\n",
    "    if overall_acc > best_acc:\n",
    "        print('New Best Accuracy {:.3f} > Old Best {:.3f}.  Saving...'.format(\n",
    "                overall_acc, best_acc))\n",
    "        best_acc = overall_acc\n",
    "        save(saver, sess, SNAPSHOT_DIR, step)\n",
    "        \n",
    "'''\n",
    "Main Training Loop\n",
    "'''\n",
    "step = 0\n",
    "epoch = 0\n",
    "best_acc = 0\n",
    "while epoch < NUM_EPOCH:\n",
    "    step += 1\n",
    "    # Allocate Space For Batch\n",
    "    x_batch = np.zeros((BATCH_SIZE,) + data_x.shape[1:], dtype=np.float32)\n",
    "    y_batch = np.zeros((BATCH_SIZE,) + data_y.shape[1:], dtype=np.int32)\n",
    "    \n",
    "    # Run Validation\n",
    "    if step % batches_per_epoch == 0:\n",
    "        epoch += 1\n",
    "        run_validation()\n",
    "        \n",
    "    # Form Training Batch\n",
    "    start_t = time()\n",
    "    for i in range(BATCH_SIZE):\n",
    "        idx = next(train_indices)\n",
    "        x_batch[i,...] = data_x[idx, ...]\n",
    "        y_batch[i,...] = data_y[idx, ...]\n",
    "    \n",
    "    # Data Augmentation\n",
    "    if random.random() < 0.5:\n",
    "        x_batch = np.fliplr(x_batch)\n",
    "        \n",
    "    # Prepare Feed Dictionary\n",
    "    feed_dict = {x:x_batch,\n",
    "                 y:y_batch,\n",
    "                 is_training: True}\n",
    "    # Run Training Summary\n",
    "    if step % SUMMARY_EVERY == 0:\n",
    "        loss_v, _, summary_v, acc_v, pred_v = sess.run(\n",
    "                [reduced_loss, train_op, all_train_summary, acc, pred],\n",
    "                feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_v, step)\n",
    "        duration = time() - start_t\n",
    "        print('step {:d} \\t loss = {:.3f}, train_acc = {:.3f} ({:.3f} sec/step)'.format(\n",
    "                step, loss_v, acc_v, duration))\n",
    "    else: # Run Simple Train\n",
    "        loss_v, _ = sess.run([reduced_loss, train_op],\n",
    "                feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
